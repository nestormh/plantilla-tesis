%%
%%  chapter02.tex - Obstacle Detection and Planning for Autonomous Vehicles based on Computer Vision Techniques
%%
%%  Copyright 2014 NÃ©stor Morales <nestor@isaatc.ull.es>
%%
%%  This work is licensed under a Creative Commons Attribution 4.0 International License.
%%

\graphicspath{{./images/chapter02/bmps/}{./images/chapter02/vects/}{./images/chapter02/}}

\chapter{Non-rigid Contour Tracking}\label{ch:chapter02}

In previous chapter, we saw a method that was able to detect obstacles in the form of changes when the images obtained with onboard cameras were compared to a set of images stored in a database. We also saw that it had several drawbacks, mainly related to the localization and tracking of the obstacles in world coordinates. In order to solve some of these problems, we decided to try a different solution which makes use of the cameras installed all around the testing area. These cameras are static monocular cameras. This is a first approach in which we can isolate the tracking problem from the rest of the problems by doing the tracking of a set of obstacles that are detected through foreground extraction methods.
The idea behind this method is having complete information of the trajectory followed by each point belonging to the contour of these obstacles. But the peculiarity of our method is that that tracking is done in a non-rigid way, so we can have information not just about the moving of a certain obstacle but also about the local movement of each part of its body.
Also, as cameras are static and point to a planar surface, we assume that all obstacles lie on the ground, so the localization of them is quite straightforward. Thanks to that, we have solved the problems found for the method described in the previous chapter for an scenario in which static cameras are available. 

The pipeline of the method described in this chapter is shown at figure \ref{fig:cp02_pipeline}. First, we use a method for foreground extraction in which we segment the moving objects from the background. From these objects we get the points that belong to its contour in the image. Then, a nonrigid point set registration algorithm is used to match the point pairs between each pair of frames. Concatenating these matches, it is quite easy to keep an structure in which the trajectory followed by each point in the current frame is represented. Finally, obstacles and tracks are located in world coordinates.

\begin{figure}[h!]
  \centering
  \includegraphics{pipeline}
  \caption{Pipeline of the tracking method described in this chapter.}\label{fig:cp02_pipeline}
\end{figure}

\section{Method}\label{ch:chapter02_01}

As seen, the method pipeline can be divided into two main steps: generation of the point sets and contour flow detection. In the first one, we try to segment the foreground of the scene to extract the moving objects. Then, boundaries of the silhouettes of these objects are obtained and a non-rigid point set registration method is used to align the borders of the objects along the sequence. Finally, we obtain the trajectory followed by each part of the contour of the object through the time.

In the following subsections, the different stages of the algorithm are described in detail, as well as the different methods that we considered as possible candidates to perform these steps.

\subsection{Generation of the point sets}\label{ch:chapter02_01_01}

The objective of this step is the segmentation of those parts of the image that do not belong to the background. This stage is critical: the silhouettes must be similar enough between frames to ensure that the matches between contours are correct and to help the method to converge in just a few iterations. Also, despite some of the nonrigid point set registration techniques described later are able to deal with noise, it is desirable to get silhouettes as clean as possible. From these silhouettes, the point sets belonging to the contour are extracted so, from the current image, we obtain the point set $Y$, which will be registered with the point set obtained from the previous frame, $X$.

\subsubsection{Foreground extraction algorithms}\label{ch:chapter02_01_01_01}

For the extraction of the foreground, there are several approaches suitable for our method and able to extract the objects without having a model of the objects we want to extract, one of the objectives desired for our method. For this purpose, we have studied four different methods, described in \cite{lopez2011stochastic}, \cite{lopez2011foreground}, \cite{guo2011hierarchical} and \cite{reddy2012improved}. In this section, we do a brief introduction of these methods. In section \ref{ch:chapter02_02_01}, they are compared using a dataset for which the ground-truth is available.
The studied foreground detection methods are the following:

\begin{itemize}
\item \textit{BACKground Stochastic Approximation (BACKSA)}: this method is based on the work described by 
\cite{lopez2011stochastic}. This method proposes an statistical approach for background modeling and foreground 
segmentation. To do that, a mixture of probabilistic models at each pixel location is used for modeling the background and 
foreground distributions, respectively. For the background, a Gaussian distribution is used, while the foreground is 
modeled by a Uniform distribution. The initial model is afterwards fine tuned using a learning algorithm. This 
learning algorithm based on the Robbins-Monro stochastic approximation has a low computational complexity, making this method suitable for real time applications.
\item \textit{Foreground detection with Self-Organising Maps (FSOM)}: this method is described at 
\cite{lopez2011foreground}. It is an interesting Artificial Neural Network based alternative. In this 
work, background is modeled through probabilistic self-organising maps. This allows modeling background pixels with 
more flexibility. A statistical correlation measure is also used to test the similarity among nearby pixels. This 
measure provides feedback to the process, enhancing the detection performance.

First of all, a model must be defined. As mentioned above, it is based on a probabilistic self organising map, which 
accepts input pixels of any color space with a space dimension equal to 3. The likelihood of the observed data (pixel 
color value) at a given pixel position is modeled by a two components mixture probability density function.
Then, model parameters are learned again by means of Robbins-Monro stochastic approximation algorithm. A method for avoiding 
suboptimal solutions based on the correlation measurement, is also used.
\item \textit{Hierarchical ForeGround detector (HierFG)}: This is described at \cite{guo2011hierarchical}. It uses a 
hierarchical scheme supported by block-based and pixel-based codebooks. Block Truncation Coding (BTC) scheme (a 
traditional compression scheme) is extended,by dividing the image into non-overlapping blocks and representing them with 
a set of 12 intensity values. With the block-based background modeling, foreground is efficiently detected, but with a 
low precision. The pixel-based codebook strategy enhances this accuracy, with a low computational cost. Also, 
this hierarchical structure allows a good performance even if the background is not completely still.

All of this solve the problem of non-stationary background, global illumination changes, while obtaining good 
results in detection and precision.

A color model is used, which allows distinguishing if a non-background pixel is indeed a shadow, highlight or 
foreground. This color model can be changed. In the tests, the model used was the same described in the original work 
\cite{carmona2008new}, as it gave good results. 
Finally, a short-term information model is used to overcome the non-stationary background problem. All these advantages make this method highly suitable for the purposes of the foreground segmentation stage, which is supported by the tests described at section \ref{ch:chapter02_02_01}. Based on these tests, a block size of $12\times12$ was used.
With this method, silhouettes are extracted. As said, the method is able to detect foreground, highlights and shadows. Just the foreground mask is needed.
\item \textit{Block-based Classifier Cascade with Probabilistic Decision Integration (BCCPDI)}: The last method 
considered is based on the work of \cite{reddy2012improved}. In the previous approach, we introduced a hierarchical 
algorithm in which data is analyzed from two different viewpoints, which are region and pixel levels. This methodology 
tackles frames as composed by a number of overlapping blocks.

The method has four main stages:
\begin{itemize}
 \item In the first stage, a given image is divided into overlapping blocks. For each block, a low-dimensional texture descriptor is generated, and it is passed to a cascade classifier.
 \item Each block is classified as background or foreground using the cascade classifier, comprising three 
classifiers, each one managing a different problem: the first one is able to handle dynamic backgrounds, but 
fails in presence of illumination variations; the second one detects whether the anomalies found in the previous 
descriptor are generated by illumination variations; and the third descriptor is able to exploit temporal correlations. 
By doing that, it is able to partially handle changes in environmental conditions while reducing spurious false 
positives.
 \item If a portion of each image is consistently classified as foreground for a reasonable period of time, the 
stage "model reinitialization" is triggered. This happens when a sudden and significant scene change is too sudden or 
too severe for the adaptation and classification strategies being followed. This can make the current background model 
inaccurate, which leads to the detection of a very large portion of the image as foreground.
 \item In the final stage, a probabilistic approach for the generation of a foreground mask is used. This approach 
exploits the block overlaps in order to integrate the decisions at block-level, obtaining a pixel-level final 
foreground segmentation. In this stage, each pixel is classified as foreground only if a significant proportion of the 
blocks that contain that pixel are classified as foreground. This strategy minimizes the number of errors in the 
output. This is in contrast to conventional methods, such as those based on Gaussian mixture models, kernel density 
estimation 
and codebook models, which do not include this correction stage.
\end{itemize}

This method also presents some advantages that make it very suitable for our application:
\begin{itemize}
 \item First of all, as happened with the previous method, it is able to deal with illumination changes and dynamic backgrounds. The use of a cascade classifier makes it very robust, as in the second classifier it is able to distinguish if the detected change was motivated by a real movement of a foreground object or it was caused by a change in the illumination. The remaining false positives are removed by the third classifier.
 \item In second place, soft contours are found. As shown in section \ref{ch:chapter02_02_01}, these contours are pretty much softer than those obtained by other methods. As the presented application is very sensitive to a bad detection of these contours, this is a key feature.
 \item It is able to work even with dynamic backgrounds. As the application will be working outdoors, this is also an important feature. This will allow the final application to work in areas where there are trees or other non static objects that could make the method fail.
 \item It works even in presence of noise. This is a good feature, as it will make the present method stronger.
\end{itemize}
 
\end{itemize}

In the final version of the application, and after performing the tests described at section 
\ref{ch:chapter02_02_01}, the algorithm chosen for the foreground detection was BCCPDI. It was not an 
clear decision, and in fact, some of the other methods, like FSOM, could be used instead.

\subsubsection{Point set extraction}\label{ch:chapter02_01_01_02}

The last step in this stage is obtaining the points that compose the contour of the objects represented in the mask. This is done using the method described at \cite{suzuki1985topological}. With this method, a new point cloud is obtained, so we have now two point sets. The first one, obtained in the previous frame, is $X$, the second one is the set $Y$. In the next step, an algorithm will be used to perform a non-rigid registration between $X$ and $Y$, which will allow matching each point $x \in X$ to its corresponding point $y \in Y$. After this registration, a matching process will be performed. The results of this matching, through the frames, will return the trajectory followed by each point in the contour of the tracked objects.

\subsection{Contour flow detection}\label{ch:chapter02_01_02}

At this point, at each frame, a pair of point sets is available. Both sets, obtained in the previous steps represent a 
pair of shapes (or a pair of shape sets, in case there is more than one object in the scene). Also, these shapes should 
be similar enough to be able to find correspondences between the different points that belong to the contours by using 
their spatial configuration. 

Assuming that there will not be a big change between frames, it is permissible to think that this condition will be 
fulfilled, and that it is possible to match the contour points by using a non-rigid point set registration based method, 
without the need of visual information.

\subsubsection{Non-rigid point set algorithms}\label{ch:chapter02_01_02_01}

In the literature, some non-rigid point set registration are available. In \cite{myronenko2010point}, a complete review of both rigid and non-rigid existing methods is enumerated. For the development of the application described here, some of them have been tested, including some others:

\begin{itemize}
 \item \textit{Iterated Closest Point (ICP) \cite{besl1992method}, \cite{feldmar1996rigid}}: In fact, the method tested 
is not the original ICP algorithm, but an improved version of it. This version is thought to allow non-rigid surface 
registration. The method is divided by three different steps: in the first step, the best rigid displacement is searched 
in order to superpose both point sets; then, the best affine transformation is obtained. In this step, a global 
criterion minimized by extended Kalman filtering is used in order to ensure the convergence of the algorithm.; finally, 
each point is attached to a local affine transformation with the purpose of deforming the point set at a local level. 
The used version of the ICP algorithm is that provided by Chui and Rangarajan \cite{chui2000new}, which also uses 
Thin-Plate Splines and includes a dynamic thresholding mechanism, as well as the use of a Gaussian distribution in order 
to fit the distances between the nearest point pairs.
 \item \textit{Thin-Plate Splines Robust Point Matching (TPS-RPM) \cite{chui2000new}}: This approach uses a Thin-Plate Spline (TPS) kernel that contains the information about the point-set's internal structural relationships and generates a non-rigid warp.
 It alternates the update of the mapping and correspondence parameters, in the way that both solutions mutually improve one another during the process until convergence is reached. In order to ensure that the method converges towards a solution, instead of getting worse, two techniques are used: the first one is soft assign, which consists on employing a variable continuous in the space inside the interval [0, 1], instead of a binary one; the other technique is called deterministic annealing, closely related to simulated annealing and described in \cite{geiger1991parallel}.
 \item \textit{Coherent Point Drift (CPD) \cite{myronenko2010point}}: It is a multidimensional probabilistic robust registration method that can be used for the matching of both rigid and non-rigid point sets. This algorithm defines the alignment of two point sets as a probability density estimation problem, where a point set represents the centroids of the Gaussian Mixture Model (GMM) and the other represents data points. Coherent Point Drift considers the alignment of two point sets as a probability density estimation problem, where one point set represents the model points and the other represents the data points. At the end of the process, the two point sets are aligned and the correspondence is obtained using the maximum of the GMM posterior probability for each point. The underlying idea of the method is to force GMM centroids to move coherently as a group to preserve the topological structure of the point sets.
 This algorithm has some advantages that make it suitable for the proposed application:
 \begin{itemize}
  \item It is able to register non-rigid large data sets. The images used in this application are not too big and just 
points in the contours are being used in order to not to increase too much the number of points. On the other hand, its 
performance is much more constant than that obtained by the other algorithms, as shown in figure 
\ref{fig:cp02_chartsRegistration}.
  \item The estimation of the GMM width reduces the number of free parameters while reducing the number of iterations and the processing time. As situations can change, it is not desirable to use a parameter dependent algorithm. This also make it easier to tune the whole application.
  \item It is able to simultaneously find both the transformation and the correspondence between two point sets without a previous assumption of the non-rigid transformation model except that of motion coherence. This makes unnecessary the use model of the objects that are expected to appear in the surveyed area.
  \item Compared to the other methods, it shows a more robust and accurate performance with respect to noise, outliers, 
and missing points. In section \ref{ch:chapter02_02_02}, it showed to have the best compromise between error, 
number of mismatches and computing time stability.
 \end{itemize}
 However, CPD assumes no specific point correspondence other than the one derived from the euclidean distances between 
points. Therefore, there are some situations where its performance can be reduced, because the likelihood function may 
not have a well-defined maximum. Also, sometimes the use of a Gaussian Mixture Model (GMM) may be a bad choice when the 
data is imperfect (i.e., the presence of outliers or missing data), leading to fake global maximum (a local maximum) in the likelihood function. Because of that, the 
algorithm can be weak in these circumstances. For example, we could find outliers in case of a bad foreground segmentation. In this case, two masks related to the same object in consecutive frames are two different so it is difficult for the algorithm to match both contours. However, in practice, the presented foreground segmentation methods are strong enough to give similar masks in consecutive frames. We might find small errors, but they are corrected in the next frame, so experimentally it seems not to be a big deal. About missing points, this effect is more common. For example, think on an arm that disappears behind the body of a person. In this case, we will not be able to match the points belonging to this arm. Anyway, this situation will be normalized for the next frame.

 \item \textit{Robust Point Matching for Non-Rigid Shapes (RPM-NRS) \cite{zheng2006robust}}: In this solution, each 
point is considered as a node in a graph. Two nodes are connected by an edge only if they are neighbors. Based on this, 
the matching problem becomes a maximization problem. This is a hard discrete combinatorial problem, and the way used to 
solve it is through relaxation labeling.
 \item \textit{Dynamic Programming based Point Set Matching (DPPSM) \cite{lian2012rotation}}: In this approach, the 
shape context (SC) descriptor is used. This descriptor is very discriminative and quite robust to various types of 
disturbances, which proves to be a useful feature for this kind of methods. In this method, a graph is also 
constructed from a point set. Edges in this graph are used to determine the orientations of SCs. Similar to lengths or 
directions, oriented SCs constructed this way can be regarded as the attributes of edges. By doing that, rotation 
invariance can be achieved by matching the edges between the two point sets. As happened with the RPM-NRS, the point set 
matching problem becomes a graph matching problem, which facilitates the use of dynamic programming for optimization.
 The key is on how the graphs are constructed for both datasets. For the data point set, a complete graph is used, in 
the way that any two points are adjacent in the graph. In the case of the model point set, two different approaches are 
explored:
 \begin{itemize}
  \item \textit{Minimum Spanning Tree induced triangulation (MST)}: In this approach, there are two types of edges: the frame edges formed by choosing one point as the reference and connecting it to the rest of points; and the boundary edges generated in the MST constructed by the model points except for the reference point.
  The graph generated is a 2-tree \cite{west2001introduction}, which allows using a dynamic programming approach.
  \item \textit{Star graph (STAR)}: As MST scheme has a relatively high time complexity, authors of the method proposed an alternative scheme, which involves only the frame edges. Edges in a star graph form a tree, which still allows using dynamic programming to find the best embedding of the graph in the data point set, but with a much lower time complexity.
 The main advantage of this algorithm is its robustness to various types of disturbances, in special clutters. This is due to the strong discriminative nature of SC, combined with the special structure described for the model graphs and the global optimality obtained through dynamic programming.
 \end{itemize}
\end{itemize}

The named methods have been tested, obtaining the results shown at section \ref{ch:chapter02_02_02}. As it will be explained better in that section, best results were obtained for the Coherent Point Drift algorithm, as it was the only one which obtained good results in all parameters tested.

\subsubsection{Points matching}\label{ch:chapter02_01_02_02}

Now that we know which is the best algorithm for the problem we are tackling, the process is the following: we use 
the Coherent Point Drift (CPD) method to align both $X$ and $Y$ point sets. After that, we have a set 
of pairs $\Phi = \{ \phi(x,y) ~|~ x \in X, ~y \in Y \}$, in which the euclidean distance of each pair of points in 
$\Phi$ can not be bigger than a given threshold $\tau$. In our tests, $\tau = 1\,px$. After that, and to ensure that all correspondences are right, 
they are filtered in order to avoid that a point belonging to an object is matched with a point from other different 
object. 
The way in which this is done is the following:
\begin{itemize}
 \item First, both $X$ and $Y$ are clustered into different objects using the method in chapter 6.2 of 
\cite{rusu2009semantic}. Then we obtain two sets of point clusters: $\mathcal{C_X} = \{ X_i ~|~ i=1 \dots M \}$ 
and $\mathcal{C_Y} = \{ Y_j ~|~ i=j \dots N \}$.
 \item For each cluster $X_i$ we define a squared region $Rx_i$ delimited by the points $(x_{min}, y_{min})$ and $(x_{max}, y_{max})$, where $x_{min}$ and $y_{min}$ are the minimal $x$ and $y$ values in the set $X_i$ and $x_{max}$ and $y_{max}$ the respective maximal values. Same process is performed for each cluster $Y_j$, obtaining the region $Ry_j$.
 \item As they do in \cite{siebel2003design}, we compare each region $Rx_i$ with each region $Ry_j$, obtaining the following values: absolute positional difference in $x$ and $y$ direction ($\varDelta x$ and $\varDelta y$, respectively), absolute difference in width ($\varDelta w$), and absolute difference in height ($\varDelta h$). From these values we obtain the following matching score:
 \begin{equation}\label{eq:matching_regions}
  \delta (Rx_i, Ry_j) = \alpha_1 \varDelta x + \alpha_2 \varDelta y + \alpha_3 \varDelta w + \alpha_4 \varDelta h.
 \end{equation}
 As done in \cite{siebel2003design}, the values used for the parameters were $\alpha_1 = \alpha_1 = \alpha_3 = 1$ and $\alpha_4 = 1.5$. The weight used for the height of the tracked objects is slightly bigger because it is usually more invariant to changes in orientation.
 \item Using the score of equation \ref{eq:matching_regions}, we look for each region $Ry_j$ the nearest region $Rx_i$. 
If the score is below a threshold $\delta_{max}$, $Y_j$ will be labeled with the label $\mathcal{L}(i) = j$.
 \item Once we have all regions labeled, we reject the pairs $\phi(x,y)$ in which the label of the cluster containing 
$x$ is not the same of the cluster containing $y$. In figure \ref{fig:cp02_clusterization_output}, it is possible to see a 
pair of objects crossing each other in a sequence. Each color represents a different cluster. In that sequence, two 
clusters are joined and, once the men in the image go in a different direction, the cluster is split again into two 
different clusters. In our tests, $\delta_{max} = 100$.
\end{itemize}

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \caption{~}
                \includegraphics[width=\textwidth]{fig1.jpg}
                \label{fig:cp02_cluster1}
        \end{subfigure}%        
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.32\textwidth}
                \centering
		\caption{~}
                \includegraphics[width=\textwidth]{fig2.jpg}
                \label{fig:cp02_cluster2}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.32\textwidth}
                \centering
                \caption{~}
                \includegraphics[width=\textwidth]{fig3.jpg}
                \label{fig:cp02_cluster3}
        \end{subfigure}%
        \caption{Clustering of objects before, during and after two of them cross each other in a sequence.}\label{fig:cp02_clusterization_output}
\end{figure*}

Pairs must be added to a structure that represents the current set of trajectories. Such structure is depicted in figure \ref{fig:cp02_trajectories_structure}. For each pair $\phi(x,y)$, we look for a point $p_{xn}$ in the list of points that are the head of the trajectories already stored in the structure, such that $p_{xn} = y$. If we found this point in the structure, we add it to the head of the associated trajectory, which is indexed through the new point $(a)$. Else, the point is added as a new trajectory $(b)$. Once all pairs have been processed, those which have not been updated are removed from the structure (represented in red in figure \ref{fig:cp02_trajectories_structure}).

\begin{figure*}[h]
  \centering
%   \includegraphics[width=0.4\textwidth, trim=180 10 100 20,clip]{finalPath.jpg}
  \includegraphics[width=0.7\textwidth, trim=0 0 0 0,clip]{fig4.pdf}
  \caption{Trajectories structure.}
  \label{fig:cp02_trajectories_structure}
\end{figure*}

At this point, we have all the trajectories generated and stored in a structure. This information can be used as 
an input for other algorithms, i.e. for human behavior recognition, video surveillance tasks or human-computer 
interaction applications. In figure \ref{fig:cp02_videoCaptures} we show a set of frames extracted from \notsure{the attached video 
named \textit{Method pipeline}}, in which it is possible to see the a representation of the output obtained by our 
algorithm. For the sake of clarity, each trajectory is represented by a random different color, and not all of them 
are depicted. Images are part of the dataset used in \cite{berclaz2011multiple}, available on the 
web\footnote{\url{http://cvlab.epfl.ch/data/pom}}.

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
                \caption{~}
                \includegraphics[width=\textwidth, trim=6 0 5 1, clip]{fig5.jpg}
                \label{fig:cp02_videoCapture1}
        \end{subfigure}%        
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
		\caption{~}
                \includegraphics[width=\textwidth, trim=6 0 5 1, clip]{fig6.jpg}
                \label{fig:cp02_videoCapture2}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
                \caption{~}
                \includegraphics[width=\textwidth, trim=6 0 5 1, clip]{fig7.jpg}
                \label{fig:cp02_videoCapture3}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.24\textwidth}
                \centering
                \caption{~}
                \includegraphics[width=\textwidth, trim=6 0 5 1, clip]{fig8.jpg}
                \label{fig:cp02_videoCapture4}
        \end{subfigure}%
        \caption{Captures from the attached video corresponding to the frames 1695, 1715, 3669 and 3789 of the dataset used by \cite{berclaz2011multiple}}\label{fig:cp02_videoCaptures}
\end{figure*}

\subsection{Localization}\label{ch:chapter02_01_03}

As we are just using a single camera for the detection and tracking of the obstacles, it is not possible to know the exact place in which the obstacle is in the real world. Fortunately, the used cameras are static, and it is possible to take advantage of this fact. As the area covered by each camera is known and tit is assumed to be planar, a set of four virtual points can be defined over the road, for which the coordinates in the map are well known.
Having this, these points can be selected on the image. By doing a perspective projection, the objects in the image can be localized on the map. To make it possible, we assume that every object is lying on the ground, touching it.

\section{Experimental results}\label{ch:chapter02_02}

In this section, four different groups of experiments have been performed. Two of them aiming at choosing the best 
algorithm combination to be used in the stages of foreground segmentation and contour flow selection. Then, we fine 
tune those algorithms, in terms of parameter values to be used. In the last group of experiments, we compare the output 
of our method with the output obtained by a Microsoft Kinect\textregistered and by a vision based method.

\subsection{Foreground segmentation}\label{ch:chapter02_02_01}

For the evaluation of the methods taken into account for the foreground segmentation stage, the accuracy metrics considered where the \textit{Recall}, \textit{Precision}, \textit{$F_1$} and \textit{Similarity} \cite{maddalena2008self}, which are used as follows:

\begin{equation}\label{eq_Recall}
Recall = { tp \over { tp + fn } }
\end{equation}
\begin{equation}\label{eq_Precision}
Precision = { tp \over { tp + fp } }
\end{equation}
\begin{equation}\label{eq_F1}
F_1 = { {2 * Recall * Precision} \over {Recall + Precision} }
\end{equation}
\begin{equation}\label{eq_Similarity}
Similarity = { tp \over { tp + fn + fp } },
\end{equation}

where $tp$, $tn$, $fp$ and $fn$ denote the number of the true positives, true negatives, false positives and false negatives, respectively. $(fp + tn)$ indicates the total number of pixels representing the background in an image, while $(tp + fn)$ is the total number of pixels representing the foreground.
These measures where tested using the sequences of SZTAKI Surveillance Benchmark Set \cite{benedek2008bayesian}, obtaining the results shown in figure \ref{fig:cp02_chartsFG}.

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 50,clip]{fig9.pdf}
                \caption{Recall}
                \label{fig:cp02_recallChart}
        \end{subfigure}%        
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 50,clip]{fig10.pdf}
                \caption{Precision}
                \label{fig:cp02_precisionChart}
        \end{subfigure}%
        
%         ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 50,clip]{fig11.pdf}
                \caption{$F_1$}
                \label{fig:cp02_f1Chart}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 50,clip]{fig12.pdf}
                \caption{Similarity}
                \label{fig:cp02_similarityChart}
        \end{subfigure}
        \caption{Values obtained for foreground detection tested algorithms}\label{fig:cp02_chartsFG}
\end{figure*}

\begin{table}[t]
\begin{center}
\caption{Foreground detection methods. Average results}\label{table:fgAverage}
\resizebox{0.5\columnwidth}{!} {
\begin{tabular}{|l|c|c|c|c|}
\hline
Method & Recall & Precision & $F_1$ & Similarity \\
\hline
BACKSA & \textbf{76.72} \% & 93.92 \% & 99.82 \% & 93.77 \% \\
FSOM & 55.29 \% & 96.88 \% & \textbf{99.87} \% & 96.76 \% \\
hierFG & 71.89 \% & 97.15 \% & 99.69 \% & 96.87 \% \\
BCCPDI & 57.87 \% & \textbf{97.61} \% & 99.83 \% & \textbf{97.46} \% \\
\hline
\end{tabular}
}
\end{center}
\end{table}

Figure \ref{fig:cp02_recallChart} shows the percentage of images (Y-axis) that are under a certain value of recall (X-axis) 
for each algorithm. As seen, best results are obtained for the BACKSA method, followed by the hierFG algorithm.  FSOM 
and BCCPDI methods have not so good results. Having a look to the Table \ref{table:fgAverage}, this behavior is also 
reflected in the average of the measures of all the images.

In figure \ref{fig:cp02_precisionChart} it is observable that the best precision is shown for the BCCPDI, which does not 
differ too much from the results obtained for the hierFG and FSOM algorithms. It is noticeable also that, for these 
three algorithms, the lowest value is above an 87\% of precision. Also, if having a look at Table \ref{table:fgAverage}, 
the average precision values for hierFG and BCCPDI are above a 97\%.

In the chart shown at figure \ref{fig:cp02_f1Chart}, results are also quite good, being the minimum value for all the 
algorithms above an 98\%, and where the best values are obtained for FSOM, not too far from the values of the BACKSA and 
BCCPDI algorithms. Despite hierFG falls a little faster than the other three algorithms, results are also good for this 
algorithm. This behavior is reflected by the values represented in Table \ref{table:fgAverage}, where all the averages 
for this measure are above a 99\%.

Finally, in the chart depicted in figure \ref{fig:cp02_similarityChart}, hierFG, FSOM and BCCPDI show a similar behavior, 
although the response of BCCPDI is slightly better. Measures for BACKSA are not so good, which practically a 60\% of the 
tested images are below a 95\% of similarity. Again, Table \ref{table:fgAverage} confirm these data, showing that best 
results are obtained for BCCPDI.

As seen, results are quite similar. In Table \ref{table:fgAverage}, it is possible to see that hierFG is the most 
constant, with high values for all the measures. However, BCCPDI is a good candidate also, despite of the not so good 
recall. There is not a clear candidate, so final decision must be performed based on the aspect of the retrieved masks. 
In figure \ref{fig:cp02_fgMasks} it is possible to see several examples of the obtained images.

\begin{figure*}
        \centering
        \begin{subfigure}[b]{0.19\textwidth}
                \centering
                \includegraphics[width=\textwidth]{fig13.jpg}
                \caption{Original}
                \label{fig:cp02_originalMask}
        \end{subfigure}%        
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.19\textwidth}
                \centering
                \includegraphics[width=\textwidth]{fig14.jpg}
                \caption{BACKSA}
                \label{fig:cp02_backsaMask}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.19\textwidth}
                \centering
                \includegraphics[width=\textwidth]{fig15.jpg}
                \caption{FSOM}
                \label{fig:cp02_fsomMask}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.19\textwidth}
                \centering
                \includegraphics[width=\textwidth]{fig16.jpg}
                \caption{hierFG}
                \label{fig:cp02_hierFGmask}
        \end{subfigure}%
	~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.19\textwidth}
                \centering
                \includegraphics[width=\textwidth]{fig17.jpg}
                \caption{BCCPDI}
                \label{fig:cp02_bccpdiMask}
        \end{subfigure}%

        \caption{Values obtained for the foreground extraction tested algorithms}\label{fig:cp02_fgMasks}
\end{figure*}

As can be seen, the cleanest images are obtained for the FSOM and BCCPDI algorithms. Also, borders of the silhouettes 
are softer, making them suitable for the proposed algorithm. However, FSOM seems to show holes in some of the frames. 
Surprisingly, the noisiest one is hierFG, which in contrast is the one that in table \ref{table:fgAverage} seemed to be 
the most stable. The image could be cleaned after post-processing it, but nevertheless, it still seems to be more 
sensitive to moving backgrounds.

Based on the results, the most promising algorithms are FSOM and BCCPDI. We finally decided to use BCCPDI for our method 
as FSOM produces holes in some frames, which is not appropriate behavior for the application.

\subsection{Nonrigid point set matching}\label{ch:chapter02_02_02}

In order to select the best of the tested algorithms, several tests were performed using the data provided by 
\cite{chui2000new}. Using this dataset, the error of the obtained registration and the number of mismatches obtained for 
each test was calculated. Also, as the bottleneck of this application is in the point set registration algorithms, we 
developed a test of the computing time needed by each algorithm.

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 50,clip]{fig18.pdf}
                \caption{Error}
                \label{fig:cp02_errorChart}
        \end{subfigure}%     
%         ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 50,clip]{fig19.pdf}
                \caption{Mismatches}
                \label{fig:cp02_mismatchesChart}
        \end{subfigure}%
        
%         ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 45 80 50,clip]{fig20.pdf}
                \caption{Times}
                \label{fig:cp02_timesChart}
        \end{subfigure}%

        \caption{Values obtained for nonrigid point set registration tested algorithms}\label{fig:cp02_chartsRegistration}
\end{figure*}

In the chart of figure \ref{fig:cp02_errorChart}, it is possible to see the error percentage made by each test. This 
error value is calculated as follows:

\begin{equation}\label{eq_errorCalculation}
error = { { \sqrt{ \sum_{i=1}^N \| p_i - t_{corresp(i)}\| } } \over N }
\end{equation}

where $N$ is the number of matched points (unmatched points are not considered), $corresp(i)$ is the index of the correspondence in the ground truth dataset for the index $i$ in the transformed model data set, and $p$ and $t$ are points in the transformed model and ground truth datasets.
In the chart of figure \ref{fig:cp02_errorChart}, it is possible that the best error rate is clearly found for the RPM-NRS 
algorithm, being followed by CPD.  The rest of algorithms present a similar behavior being, from the best to the 
worst, TPS-RPM, ICP, DPPSM-MST and DPPSM-STAR. This is expectable, as the STAR is a simplification of MST in order to 
have a better time performance.

About the percentage of mismatches, in figure \ref{fig:cp02_mismatchesChart} it is possible to see the results obtained. In 
this chart, the percentage of mismatches is shown. The number of mismatches is calculated using a ground truth, and the 
percentage is obtained by dividing this value by the number of points in the smaller point set. The best algorithm in 
this case is TPS-RPM, followed again by CPD. However, RPM-RNS, which presented the best results in the previous test, 
has the worst results in this case. The relative behavior between ICP, DPPSM-MST and DPPSM-STAR in this test is 
similar to that obtained in the previous one.

Finally, in figure \ref{fig:cp02_timesChart} it is possible to see the relative time needed by each algorithm, with respect to 
the maximal time obtained in all the tests. This value is shown ordered. In this case, the more time consuming is 
DPPSM-MST. As expected, DPPSM-STAR is faster. The difference in efficiency shown in this chart demonstrates that worst 
results in the previous tests are compensated by a faster algorithm. RPM-NRS shows a high speed in some tests, but it is 
one of the slowest in the others. 

Analyzing data, it is due to the fact that the fastest results are obtained for clean 
tests, without outliers or noise. Once the noise and outliers starts increasing, the algorithm becomes slower.
The most interesting behavior is that shown by ICP and CPD algorithms, as they remain constant. This is a good 
symptom, it means that the computing time of that algorithms is independent on the input data. This is a very desirable 
feature for the presented application. In fact, CPD developers \cite{myronenko2010point} claim that their algorithm is 
the only one able to work with large point sets.

In this case, the choice of methodologies to the final tests was easy. CPD clearly showed to have the best compromise 
between number of mismatches, error and time consumption.

\subsection{Final algorithm evaluation}\label{ch:chapter02_02_03}

In this section, the final method is evaluated. At the beginning of the section, we explain the method used to perform the error measurements. Then, an study of the best parameters to be used in the CPD algorithm is performed. Finally, we compare our method with other approaches.

\subsubsection{Method evaluation methodology}\label{ch:chapter02_02_03_01}

In this section, we want to evaluate the performance of our application. The main problem is that there is not a 
database available in which there is a ground truth information about the correspondences of the contours of objects in 
an scene along the time. Because of that, we had to create an indirect way to measure the performance of our method. The 
way in which we did that was by recording a sequence in which a person moves each part of its body using a Microsoft 
Kinect \textregistered, and process it to obtain the skeleton using the method described in \cite{shotton2013real}, 
which is implemented in the OpenNI libraries\footnote{\url{http://www.openni.org/}}.

The idea is the following: for each frame at time $t$, we have an initial skeleton provided by the Kinect, which is used as the initial configuration. From this skeleton, we repeat the following process for each joint $s_i$ in the skeleton $S$:
\begin{itemize}
 \item First, we look, for the contour of the silhouette extracted in the current frame, the joint nearest points. To do that, we create the set of points $\mathcal{D} = \{ x \in X \cup s_i \}$, and the Delaunay triangulation \cite{lingas1994linear} is applied to them, as shown in figure \ref{fig:cp02_err_measure_triangulation}. This triangulation is also a visibility graph in the sense that we are going to select the subset of points $X' \subseteq X$ in which all the points in $X'$ have a line joining them with the joint $s_i$. It is more clear in the left image of figure \ref{fig:cp02_err_measure_trilateration}. For each point in $X'$, we also obtain the set $D = \{ x'_k - s_i) | x'_k \in X' \} $.
 \item For each $x'_k \in X'$, we obtain the set of correspondences $Y' \subseteq Y$ for the new frame $t + 1$, which is 
obtained using the CPD algorithm. Using this new set and the set $D$, we can obtain the new position $s'_i$ by 
trilateration. As we are working in 2D, in this case we can separate the $x$ and $y$ components in order to have two 
separate linear problems in which we have one variable and more than one equation. For instance, for obtaining the 
value of $s'_i(x)$, we solve the following system of equations:
 
 \begin{equation}
  \left( \begin{array}{cc}
1 & x'_0 \\
\vdots & \vdots \\
1 & x'_k \\
\vdots & \vdots \\
1 & x'_K \end{array} \right)
  \left( \begin{array}{c}
s'_i(x) \\
1 \end{array} \right) = 
  \left( \begin{array}{cc}
d_0 \\
\vdots \\
d_k \\
\vdots \\
d_K \end{array} \right)
 \end{equation}
 
In this equation system, $K$ is the number of points in $X'$. A similar process is performed for $s'_i(y)$. In 
figure \ref{fig:cp02_err_measure_trilateration} it is possible to see, in the left, the initial $s_i$ corresponding to the 
\textit{torso} joint with the relative points of set $X'$. In the right side, the generated $s'_i$ together with the 
respective $Y'$ set.
\end{itemize}

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=0 0 0 0,clip]{fig21.jpg}
                \caption{Triangulation}
                \label{fig:cp02_err_measure_triangulation}
        \end{subfigure}%        
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=40 230 30 220,clip]{fig22.pdf}
                \caption{Trilateration}
                \label{fig:cp02_err_measure_trilateration}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=0 0 0 0,clip]{fig23.jpg}
                \caption{New joints obtained}
                \label{fig:cp02_err_measure_new_joints}
        \end{subfigure}%

        \caption{Skeleton generation process for the evaluation of the method}\label{fig:cp02_err_measure_skeleton}
\end{figure*}

The evaluation of the algorithm has been performed by comparing the position of each joint after doing the process 
described above in this section with the joint generated using the Kinect. Of course, using the Kinect as ground truth 
is not the best option as it has its own error. However, we expect that if the behavior is similar using both 
approaches, the performance of the algorithm is good. For avoiding including extra error, in each frame we start from 
the same joint set $S$, provided by the Kinect. That is, for the evaluation of the method, the initial skeleton is 
provided by the Kinect, so we just evaluate its evolution as generated by our application. By doing so, we avoid biasing 
produced by a deformation of the skeleton due of a single bad frame. The measurement of the displacement is performed 
separately for each frame.
Also, we want to notice that, using the process described in this section with the trajectories obtained would allow to, 
given an initial set of joints, analyze its evolution along the time as Kinect does, but without the need of a previous 
model. It can be useful in motion analysis or virtual reality applications.

\subsubsection{Parameterization}\label{ch:chapter02_02_03_02}

In this section, we perform a study in order to know the best tuning parameters for our application. In particular, we 
want to obtain the parameters that produce the best performance from the CPD, given the sort of data we are working 
with. Parameters chosen for this study are those that are described as free parameters in \cite{myronenko2010point}: 
$\omega$, $\lambda$ and $\beta$. According to the authors of the algorithm, parameter $\omega$, which can have a value 
between $0$ and $1$, reflects the assumption on the amount of noise in the point sets; parameter $\beta$ defines the 
width of the smoothing Gaussian filter used by the method; and $\lambda$ represents the trade off between the goodness 
of maximum likelihood fit and regularization.

For this evaluation, we measure the average distance between the joints generated with the method described in section \ref{ch:chapter02_02_03_01} and the Kinect joints used as ground truth. Results of this evaluation can be observed in figure \ref{fig:cp02_err_measure_parameterization}.

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=80 80 150 100,clip]{fig24.pdf}
                \caption{$\lambda$-$\beta$ comparison.}
                \label{fig:cp02_err_measure_lambda_beta}
        \end{subfigure}%        
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=80 80 150 100,clip]{fig25.pdf}
                \caption{$\lambda$-$\omega$ comparison.}
                \label{fig:cp02_err_measure_lambda_omega}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.33\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=80 80 150 100,clip]{fig26.pdf}
                \caption{$\omega$-$\beta$ comparison.}
                \label{fig:cp02_err_measure_omega_beta}
        \end{subfigure}%

        \caption{Comparison of the error obtained when combining different parameters.}\label{fig:cp02_err_measure_parameterization}
\end{figure*}

In this figure, best results are in dark blue, and worst results are in yellow, representing high and low distances, 
respectively. In figure \ref{fig:cp02_err_measure_lambda_beta}, it is possible to deduce that best results are obtained when 
both $\lambda$ and $\beta$ are equal to $5$. It is important to evaluate both parameters together because, apart from 
the fact that both parameters are related to the smoothness regularization, the worst result is obtained precisely when 
$\lambda=5$, but $\beta=1$. However, it seems that the biggest contribution to the overall error is given by parameter 
$\beta$, except when its value is $1$. In this case, the contribution of $\lambda$ is uncertain.

The combination $\beta = 5$, $\lambda = 1$ is also a good choice, but if we have a look on figures \ref{fig:cp02_err_measure_lambda_omega} and \ref{fig:cp02_err_measure_omega_beta}, it is pretty clear that best results are obtained when $\beta=\lambda=5$, and $\omega=0.1$. This is quite surprising, as we assume that the error in the input data is quite high. However, worst results are obtained with $\omega=1$, with an average distance of more than 2.6\,pixels.

\subsubsection{Comparison with other methods}\label{ch:chapter02_02_03}

\begin{figure*}[t]
        \centering

        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 40,clip]{fig27.pdf}
                \caption{Left elbow.}
                \label{fig:cp02_comparison_left_elbow}
        \end{subfigure}%        
        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
		  \includegraphics[width=\textwidth, trim=50 40 80 40,clip]{fig28.pdf}
                \caption{Left hand.}
                \label{fig:cp02_comparison_left_hand}
        \end{subfigure}%        

        \begin{subfigure}[b]{0.5\columnwidth}
                \centering
                \includegraphics[width=\textwidth, trim=50 40 80 40,clip]{fig29.pdf}
                \caption{Left knee.}
                \label{fig:cp02_comparison_left_knee}
        \end{subfigure}%        

        \caption{Comparison of the behavior in different joints.}\label{fig:cp02_comparison}
\end{figure*}

As far as we are concerned, there is not a method in the literature that uses geometrical nonrigid point set 
registration methods for performing a contour tracking. One of the contributions of our method is that we demonstrate 
that it is possible to perform such a task by using just geometrical information. Because of that, we compare our method 
with the output of the OpenNI algorithm that makes use of the Kinect, and with a computer vision based method. About the 
former, as said above, we consider it as ground truth, because we have not found data which includes the information of 
the contour correspondences between frames at point level. 

As we will see, this method sometimes fails. Due to that, we will pay attention to the frames in which the output of 
our algorithm and Kinect's diverge, in order to know what is happening. For the tests, we have calibrated the sensor in 
the way that the skeleton generated from the depth map obtained by the Kinect device is perfectly aligned with the $2D$ 
image obtained by its visible camera.

The computer vision based algorithm works as follows: starting from the set $X$, obtained with the method described in 
section \ref{ch:chapter02_01_01}, we apply the Lucas-Kanade \cite{bouguet2001pyramidal} optical flow 
method between the current and the next frame, obtaining the set $Y$. At this point, we have a set of correspondences 
equivalent to the set $\Phi$ described in section \ref{ch:chapter02_01_02_02}. Then, the process is similar to 
that described for our method.

In figure \ref{fig:cp02_comparison}, several charts show the behavior of our method compared with the other two, for three 
representative joints. In the charts, the absolute increment is shown separately for $x$ and $y$ coordinates in the 
upper and lower charts. The variations of these increments are shown along the frames. In these charts, BCCPDI-CPD is 
our method, BCCPDI-LK is the optical flow based method, and Kinect is the OpenNI output. \notsure{The sequence, which is shown in 
the attached video with the name \textit{Algorithm evaluation}, consists on a person who moves separately each part of 
his body.} The detection of this movement is reflected in the charts. For example, left arm is moved up and down between 
frames $0$ and $100$, and again between frames $200$ and $300$, as deduced by the bigger changes shown in the charts of 
figures \ref{fig:cp02_comparison_left_elbow} and \ref{fig:cp02_comparison_left_hand}. Left leg is also moved between frames $200$ 
and $250$, as seen in the chart of figure \ref{fig:cp02_comparison_left_knee}.

In the charts, we can see that the behavior of our method is quite similar to that shown by the Kinect. In fact, it is 
quite more similar than the behavior shown by the Lucas-Kanade output. In instance, if we have a look on the set of 
frames between $80$ and $100$ of the $|\Delta X|$ chart of figure \ref{fig:cp02_comparison_left_elbow}, they have a much 
bigger increment than that shown by the other two methods. 

If we have look on the frame $96$, shown in figure 
\ref{fig:cp02_comparison_oflow_fails_elbow_lk_side}, we can observe how BCCPDI-LK fails in finding the correspondences, as 
the movement of the arm is mostly up to down. In this figure, correspondences are represented with a green line joining 
them, where the blue dots represent the points of the set $Y$. Joints are represented by the red circles, and in magenta 
we show the trajectory calculated for each joint. 

Also in figure 
\ref{fig:cp02_comparison_oflow_fails_elbow_lk_side} we can appreciate how the 
orientation of the correspondences is not the same for all of them, even when they belong to the same rigid part of the 
body. 

Likewise, if 
these results are compared with the tracking performed by the BCCPDI-CPD method, shown in figure 
\ref{fig:cp02_comparison_oflow_fails_elbow_cpd_side}, we can see that there are a lot of missing correspondences, specially 
in the upper side of the arm. A similar example is shown in figure \ref{fig:cp02_comparison_oflow_fails_knee_cpd_side} for 
the frame $226$, in which we can see what is happening in the set of frames between $210$ and $230$, for which a 
different behavior is found for BCCCPDI-LK in figure \ref{fig:cp02_comparison_left_knee}. As explained for figure 
\ref{fig:cp02_comparison_oflow_fails_elbow_lk_side}, some correspondences are missing in the upper part of the thigh and some 
others are not right.

If we have a look at the upper chart in figure \ref{fig:cp02_comparison_left_elbow}, there is a set of frames in which the 
absolute increment of $x$ between frames is similar for BCCPDI-CPD and BCCPDI-LK. As said, Kinect is not perfect and 
there are some situations in which we obtain better results. This is one of the cases. 

In figure 
\ref{fig:cp02_comparison_kinect_fails}, corresponding to frame $199$, we represent in blue the skeleton calculated by the 
Kinect method, while in green we represent the skeleton generated by our method. In these frames, the person in the 
image was performing a lateral displacement. As can be seen, the skeleton provided by our method is more centered into 
the person that the obtained using the Kinect. 

\begin{figure*}[t]
        \centering

        \begin{subfigure}[b]{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=0 0 0 0,clip]{fig30.jpg}
                \caption{BCCPDI-CPD tracking for frame $96$.}
                \label{fig:cp02_comparison_oflow_fails_elbow_cpd_side}
        \end{subfigure}%        
	~
        \begin{subfigure}[b]{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=0 0 0 0,clip]{fig31.jpg}
                \caption{BCCPDI-CPD tracking for frame $226$.}
                \label{fig:cp02_comparison_oflow_fails_knee_cpd_side}
        \end{subfigure}%        

        \begin{subfigure}[b]{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=0 0 0 0,clip]{fig32.jpg}
                \caption{BCCPDI-LK tracking for frame $96$.}
                \label{fig:cp02_comparison_oflow_fails_elbow_lk_side}
        \end{subfigure}%    
        ~
        \begin{subfigure}[b]{0.45\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=0 0 0 0,clip]{fig33.jpg}
                \caption{BCCPDI-LK tracking for frame $226$}
                \label{fig:cp02_comparison_oflow_fails_knee_lk_side}
        \end{subfigure}%     

        \caption{Some frames in which BCCPDI-LK has a poor performance.}\label{fig:cp02_comparison}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.5\columnwidth, trim=0 0 0 0,clip]{fig34.jpg}
  \caption{An example in which our method outperforms Kinect output.}
  \label{fig:cp02_comparison_kinect_fails}
\end{figure*}

\notsure{In the video named as \textit{Algorithm evaluation} it is possible to see the full sequence and the output obtained by each different algorithm.}

\section{Summary}\label{ch:chapter02_03}

In this chapter we have presented a method for the estimation of the trajectory followed by each part of a non-rigid body in sequences of static images. The key features of the proposed method are:
\begin{itemize}
 \item It can work both indoors and outdoors
 \item It does not need any previous model of the tracked object
 \item It can track each part of the body, separately, without any previous assumption.
\end{itemize}

Additionally, we think that a similar method to the process described in section \ref{sec:results_evaluation_metodology} can be used for the detection of each part of a body without a model, by taking advantage of the whole set of trajectories through the time, with just the need of an initial skeleton.

With this contribution, we have demonstrated that it is possible to perform the tracking of the contour with the use of just geometrical information. In our tests, we showed that our method works even better than optical flow in certain situations. 

As future work, we plan to study the inclusion of visual information to improve the results given by the current method.

We have studied the performance of several state-of-the-art foreground extraction algorithms, shedding light on the 
benefits and drawbacks of each one of them. BCCPDI has proved to be the one which fulfilled the most our needs. 

We also have performed some tests in order to select the best of the existing state-of-the-art nonrigid point set registration methods, demonstrating that CPD is the most stable of all them. Final tests demonstrated the good behavior of the algorithm, showing that it is possible to obtain a local tracking of nonrigid objects by just using static cameras.

However, the application has some unsolved challenges that must be tackled in future, mainly related to the 
limitation of using just the contours of $2D$ masks. The first of them is that if a  $2D$ mask is used, we lost any 
depth information, so it is impossible to find the contours of a first-plane object if in the image there is more than 
one object touching each other. 

Another limitation we have noticed is that it is impossible to track the movement of a hand, for example if it 
passes in front or behind of the body of a person. 

All these problems can be overcome with the use of $3D$ point clouds as input for the nonrigid point set registration 
algorithms obtained with an stereo pair. This can be achieved easily, as most of the presented methods are able to work 
with $3D$ data. In particular, CPD claims to be able to work perfectly with $N$-dimensional data. Plus, we could also 
use it to locate objects in the scene.

Therefore, as future work, we plan to develop a framework which uses $3D$ data obtained trough stereo vision and 
combines the named nonrigid point set registration algorithms with visual information. This would allow to avoid some limitations inherent to the method, as the need of an static background and static cameras.

In the following chapters, we will see some approaches that, in this sense, use 3D information for the detection and tracking of the obstacles, but in a different way than that followed by the method presented in this chapter. But, before that, we will see an study of the existing techniques available for stereo reconstruction related to \ac{ADAS} applications.




