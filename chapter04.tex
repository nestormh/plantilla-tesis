%%
%%  chapter05.tex - Obstacle Detection and Planning for Autonomous Vehicles based on Computer Vision Techniques
%%
%%  Copyright 2014 NÃ©stor Morales <nestor@isaatc.ull.es>
%%
%%  This work is licensed under a Creative Commons Attribution 4.0 International License.
%%

\graphicspath{{./images/chapter04/bmps/}{./images/chapter04/vects/}{./images/chapter04/}}

\chapter{Stixel World}\label{ch:chapter04}

In the previous chapter, we saw a set of algorithms and configurations able to construct a disparity map from a pair of stereo images. These methods are useful for the reconstruction of the environment. However, this reconstruction is dense, which leads to an intensive usage of the computer resources. In order to solve this problem, some authors propose approaches that try to minimize the area of the image being processed by doing a simpler reconstruction than that done with dense 3D reconstruction algorithms. In this sense, \cite{badino2009stixel} proposed to represent the world by a set of rectangular sticks named \emph{stixels} (from \emph{stick} and \emph{pixel}). Each stixel is defined by its 3D position relative to the camera and stands vertically on the ground, having a certain height.
This compact but flexible representation of the world can be used as the common basis for the scene understanding tasks of driver assistance and autonomous systems. The main advantages of using such an approach are listed next:
\begin{itemize}
 \item \emph{Compact}. Significant reduction of the data volume.
 \item \emph{Complete}. Information of interest is preserved.
 \item \emph{Stable}. Small changes of the underlying data must not cause rapid changes within the representation.
 \item \emph{Robust}. Outliers must have minimal or no impact on the resulting representation.
\end{itemize}

The method described in this chapter is based in the fast stixels implementation described in \cite{benenson2012pedestrian}. One of the main advantages of this implementation is that there is no need of obtaining a previous depth map. This fact is also the main reason for which we did not use the original implementation from \cite{badino2009stixel} or that from \cite{pfeiffer2010efficient}. Based on the output of this method, we do the tracking of the stixels following a bipartite-graph method based on a given cost metric. In this sense, we have tested several cost metrics, which are described in the following sections.

Our tracking method is based on the contribution by \cite{gunyel2012stixels}. There, movement is just computed for the areas which are covered by stixels. It has been demonstrated that stixels are good enough to do a representation of the surroundings of a vehicle, and that they have a detail level enough for the movement detection. Traditionally, this movement detection is carried out through the computation of the optical flow between two frames, which is computationally expensive. In this chapter, we will see how this movement can be computed also using the stixel world representation, by extending the method of \cite{gunyel2012stixels}. For that, we have followed two different approaches, each with its own benefits and drawbacks. The first tries to cluster the stixels in objects, so the tracking is performed at object level; the second joins the stixel tracking with the object tracking in a two level tracking scheme.

Our contribution in this work is summarized next:
\begin{enumerate}
 \item Improvement of the reconstruction obtained by the stixels. The free space computation without the use of a disparity map have some drawbacks. Some of them are related with the fact that the precision in the reconstruction of the obstacles is not good enough. The object reconstruction detection scheme proposed in this chapter allows the correction of the stixel depths and the removal of the fake obstacles, as described in sections \ref{ch:chapter04_02_01} and \ref{ch:chapter04_02_02}.
 \item Improvement of the results obtained by the method described in \cite{gunyel2012stixels}, in both object and two-level tracking approaches. We modified the method by using a graph-based approach instead of a \ac{DP} based method, as they did. 
 \item Different cost metrics for the tracking have been tested, with promising results. 
 \item Faster tracking. As we will see in section \ref{ch:chapter04_02_04}, the speed achieved with our implementation is better, specially in the object based tracking. The two-level tracking is also a little bit faster, thanks to the usage of a bipartite graph based method for the matching of the stixels between frames.
 \item Our method is slightly more robust after changes between images (for example, in presence of a low framerate), specially in the case of the just object based approach.
\end{enumerate}

In the following section, each of the steps of the method are detailed. At the end of the chapter some results are shown in order to evaluate the performance of the described method.

\section{The Method}\label{ch:chapter04_01}

The method follows a pipeline as that described in figure \ref{fig:cp04_pipeline}. First, from a given pair of images, the free space in front of the vehicle is computed in order to estimate the ground plane. From the extracted ground plane, stixels are obtained. Then, a two-level tracking approach is started. The first level is that corresponding to the tracking of the stixels. This stage is inspired by the work described in \cite{gunyel2012stixels}. The set of stixels computed for the current frame are compared with those from the previous frame. Based on this comparison, some of them are matched. At the same time, we cluster stixels based on their position once projected in the 3d world. Using the clusters and the tracked stixels, we start a new tracking step in which the tracking is performed at object level. At the end of this step, we will know the obstacles in the scene and their velocities, as well as an historic of the path followed by them that could be useful for future movement estimation.

We also propose another solution in which just the object level tracking is performed. In this case, instead of doing a comparison at stixel level, we compare each obstacle to the obstacles detected in the previous frame, so the stixel level tracking is not needed anymore. In section \ref{ch:chapter04_02}, we will see the advantages and drawbacks of using one or another approach.

\begin{figure}[h!]
  \centering
  \includegraphics{pipeline.jpg}
  \caption{Pipeline of the stixels tracking method described in this chapter.}\label{fig:cp04_pipeline}
\end{figure}

\subsection{Free space computation}\label{ch:chapter04_01_01}

In this section and in the following, we will describe the stixel extraction method, which is similar to that described in \cite{benenson2012pedestrian}. This method works under the following assumptions:

\begin{itemize}
 \item The input of the algorithm is a calibrated stereo image pair.
 \item We use the Lambertian surface assumption.
 \item The ground is planar, at least at local level.
 \item The objects of interest are mainly vertical, and have a limited height range.
 \item Stereo rig has a negligible roll with respect to the ground plane.
\end{itemize}

In this section, we introduce the first step needed for the stixels estimation process, which is the ground plane estimation stage. This ground plane is estimated using the evidences collected in the \emph{v-disparity} domain. Instead of computing and projecting a depthmap obtained, for example, one of the methods evaluated in chapter \todoref{XXX}, the evidence is collected directly from matching the rows from the left and right images at different disparities, obtaining a cube $(U, V, D)$ in which each cell represents the cost of matching the pixel at $p_i(u_i, v_i) \in I_L$ with that at $q_i(u_i + d_i, v_i) \in I_R$. Here, $p_i$ and $q_i$ are pixels from the left ($I_L$) and right ($I_R$) images, respectively; and $u_i$, $v_i$ and $d_i$ are the different possible column, row and disparity values in the cube. In figure \ref{fig:cp04_freespace}, a graphical description of the obtained cube is represented.

\begin{figure}[h!]
  \centering
  \includegraphics{freespace}
  \caption{Free space computation process.}\label{fig:cp04_freespace}
\end{figure}

For each row $v_i$, the disparity with the lowest cost is extracted for each row, as depicted in figure \ref{XXX}. From these values, a robust line fitting is used in order to find the ground plane, which is represented by the following expression:

\begin{equation}\label{eq:cp04_ground_plane_function}
  d_i = f_{ground}(v_i)
\end{equation}

Using this equation, we can know the disparity associated to each row at ground level, which will we use later for the stixels computation. For optimization purposes, instead of collecting the evidences for each row below the horizon, just one out of each $N$ rows is computed. 

\subsection{Stixels extraction}\label{ch:chapter04_01_02}

With the ground plane, we can start detecting the stixels in the images. The way in which this is done is by dividing the image in multiple row bands $b_i$. Inside each band $b_i$, and for each column $u_i$, the pixel with the biggest horizontal gradient is selected. This reduces the computational cost while the possibilities of finding the border of the object accurately are increased. Also, one of the advantages of using bands and not the rows directly is that, in presence of a confusing horizontal line, the effect is compensated by the band height.

From \ref{fig:cp04_freespace}, we obtain:

\begin{equation}\label{eq:cp04_ground_plane_function_by_band}
  d(q_j, b_i) = f_{ground}(v(q_j, b_i))
\end{equation}

Here, $q_j$ makes reference to the stixel $j$, where the total number of stixels is the total number of columns in the image divided by a parameterized stixel width $\tau_{stixel\_width}$. That is, $j=1 \dots \tau_{stixel\_width}$. In general, interest objects are wider than one column, so computing an evidence for each column in the image is redundant. Each stixel is located in the column $u(q_j) = j \cdot \tau_{stixel\_width}$. $v(q_j, b_i)$ is a certain row inside the stixel $q_j$ in the band $b_i$. 

At this point, the goal is to localize the optimal band for each stixel. This is based on the following expression:

\begin{equation}\label{eq:cp04_stixel_band_cost}
  b^*_s (q) = \underset{b(q)}{\arg\min} \underset{q}{\sum}c_s(q, b(q)) + \underset{q_a, q_b}{\sum}s_s(v(q_a, b(q_a)), v(q_b, b(q_b)))
\end{equation}

Here, we can think on $c_s$ as the data term, while $s_s$ is the smooth term. $q_a$ and $q_b$ are neighbors. That is, $|q_a - q_b| = 1$

\paragraph{Data term}\label{ch:chapter04_01_02_01}

By computing the cost $c_s$, we know the likelihood of the presence of an stixel $q$ at the row band $b$. The lower the cost is, the more possible that there is a stixel. This cost is computed as follows:

\begin{equation}\label{eq:cp04_stixel_band_cost_data_term}
  c_s(q, b) = c_o (u(q), d(q, b)) + c_g(u(q), d(q,b))
\end{equation}

This equation is composed by two terms:
\begin{itemize}
 \item \emph{Object cost($c_o$)}. It represents the cost of the presence of a vertical object. It just sums the evidence along the vertical column, using the expected height of the object, which is projected on the image using the distance given by the ground plane.
 \item \emph{Ground cost($c_g$)}. It represents the cost of a supporting ground being present, and sums the evidence along the ground plane.
\end{itemize}

\paragraph{Smooth term}\label{ch:chapter04_01_02_02}

The smooth term ($s_s$) forces to respect the left-right occlusion restrictions and promotes ground object boundaries with few jumps.

\begin{equation}\label{eq:cp04_stixel_band_cost_smooth_term}
  s_s(v_a, v_b) = 
  \begin{align*}
    \begin{cases}
    \infty & \text{if } d(v_a) < d(v_b) - 1 \\
    c_o(u_a, d(v_a)) & \text{if } d(v_a) \approx d(v_b) - 1 \\
    - \omega \cdot c_o(u_a, d(v_a)) & \text{if } q_a = q_b \\
    0 & \text{if } d(v_a) > d(v_b) - 1
    \end{cases}
  \end{align*}
\end{equation}

Here, $\omega$ is a free parameter, chosen by the user, which promotes boundaries with a few jumps. At this point, we will have extracted the stixels, obtaining a result similar to that shown at figure \ref{fig:cp04_stixels}, in which the stixels (in green) are superimposed to the left image from which they were extracted. For more information about the process described in sections \ref{ch:chapter04_01_01} and \ref{ch:chapter04_01_02}, please refer to \cite{benenson2012fast}.

\begin{figure}[h!]
  \centering
  \includegraphics{stixels_over_original}
  \caption{Stixels superimposed to the frame from which they were extracted.}\label{fig:cp04_stixels}
\end{figure}

\subsection{Tracking}\label{ch:chapter04_01_03}

At this point, we are ready to start tracking the stixels. As said before this tracking has been performed using two different approaches: one consists on two different tracking levels from the first one tracks the stixels independently (that is, each stixel at the previous frame $t - 1$ is matched with another - or none - at the current frame $t$). So it becomes a matching problem in which we try to find the minimal cost matching between frames. In \cite{gunyel2012stixels}, this minimal matching was done using \ac{DP}. In our implementation we use a bipartite graph matching based method. In a second level, stixels are clustered into objects, which are matched based on the previous stixels tracking.

In the other approach, just the second level is performed, tracking obstacles based on one of the cost metrics, without considering the stixels included in the objects (except for the clustering process). In the next sections, the tracking process is described. Please note that the process described in section \ref{ch:chapter04_01_03_01} is performed only for the two-level approach, while the process in \label{ch:chapter04_01_04} is common to both approaches, except from the differences explicitly indicated in that section.

For this stage, we made some assumptions:
\begin{itemize}
 \item First, we assume that all stixels have been properly estimated.
 \item The maximal speed of the objects is limited, so we limit the search range between stixels to a certain threshold. This range depends on the distance of the stixel at the current frame and the frame rate. As there is just one stixel per column, we can limit also the matching process to a search into the $u$ direction.
 \item There is not a big temporal difference between two consecutive frames. That means that the same stixel at time $t$ and $t - 1$ should be similar. Also their height in meters should be similar. \notsure{As shown in section \ref{ch:chapter04_02_03_02}, this temporary consistence is more or less restrictive depending on the approach finally used for the tracking process.}
\end{itemize}

\subsubsection{Stixels-level tracking}\label{ch:chapter04_01_03_01}

In this stage, the tracking is performed at stixel level. As we just try to match each stixel at column $q_i\{t\}$ with another in the previous frame $t - 1$, we can think on this process as a matching problem. The way in which we have solved this is through a bipartite graph, in which nodes are the stixels at frame $t$ and $t - 1$, and the edges are associated to a certain movement cost $c_m$, which is represented by the equation:

\begin{equation}\label{eq:cp04_stixel_movement_cost}
  c_m(u_i\{t\}, u_j\{t - 1\}) = 
  \begin{align*}
    \begin{cases}
    f_{cost}(u_i\{t\}, u_j\{t - 1\}) & \textbf{if} \text{ matching is applicable} \\
    \infty & \textbf{otherwise}
    \end{cases}
  \end{align*}
\end{equation}

Here, a matching is applicable if and only if the following restrictions are satisfied:
\begin{itemize}
 \item $|X(u_i\{t\}) - X(u_j\{t - 1\})| < \tau_{max\_disp}$, where the free parameter $\tau_{max\_disp}$ indicates the maximal displacement of a certain stixel between frames; and $X(u)$ refers to its $X$ position in 3d coordinates.
 \item $u_i\{t\}$ is not a new stixel. That is, this is not the very first frame in which the stixel appears.
 \item Stixels $u_i\{t\}$ and $u_j\{t - 1\}$ are not occluded.
 \item $f_{cost}(u_i\{t\}, u_j\{t - 1\}) < \tau_{max\_cost}$.
\end{itemize}

Else, the cost is considered as infinity, so the link is not included in the graph. $f_{cost}(u_i\{t\}, u_j\{t - 1\})$ is a cost function which can be composed by different cost metrics, weighted by a set of parameters in the form:

\begin{equation}\label{eq:cp04_cost_function}
\small
  \begin{align*}
  f_{cost}(u_i\{t\}, u_j\{t - 1\}) =~  & & \alpha_{SAD} & ~\cdot~ & f_{SAD}(u_i\{t\}, u_j\{t - 1\}) \\
      & + & \alpha_{hist} & ~\cdot~ & f_{hist}(u_i\{t\}, u_j\{t - 1\}) \\
      & + & \alpha_{height} & ~\cdot~ & f_{height}(u_i\{t\}, u_j\{t - 1\})
  \end{align*}
\end{equation}

Here, $\alpha_{SAD} + \alpha_{hist} + \alpha_{height} = 1$. $f_{SAD}(u_i\{t\}, u_j\{t - 1\})$, $f_{hist}(u_i\{t\}, u_j\{t - 1\})$ and $f_{height}(u_i\{t\}, u_j\{t - 1\})$ are the factors for the different cost metrics being tested, which are described next.

\subsubsection{\acf{SAD}}\label{ch:chapter04_01_03_01_01}

This cost is computed as the pixel-wise sum of the absolute differences over the RGB color scheme between $u_i\{t\}$ and $u_j\{t - 1\}$, following the expression

\begin{equation}\label{eq:cp04_stixel_movement_sad_cost}
\tiny
f_{SAD}(u_i\{t\}, u_j\{t - 1\}) = 
\overset{v_b(q_i\{t\})}{\underset{v_1=v_a(q_i\{t\})}{\sum}}
\overset{v_b(q_j\{t - 1\})}{\underset{v_2=v_a(q_i\{t - 1\})}{\sum}}
| I_L\{t\}(u_i\{t\}, v_1) - I_L\{t - 1\}(u_j\{t - 1\}, v_2) |
\end{equation}

As it is very unlikely for both stixels to have the exact same height, they are resized to a dimension of $30\,px$. This metric was originally used in \cite{gunyel2012stixels} for the computation of the movement of the stixels. It has been used in our tests to compare the our results with those obtained by them in their implementation.

\subsubsection{Histograms matching}\label{ch:chapter04_01_03_01_02}

As said, the same stixel has not always the same size in different frames. This can happen because the position of the object partially represented by the stixel can change its depth in the scene, or due to noise in the height detection of the stixel. In order to normalize this, we have computed the histogram of each of the stixels being compared. Then, the cost is computed from the Hellinger distance between both histograms:

\begin{equation}\label{eq:cp04_stixel_movement_histograms_cost}
f_{SAD}(u_i\{t\}, u_j\{t - 1\}) = 2 \cdot \sqrt { 1 - \underset{i=1}{\overset{d}{\sum}}\sqrt{H(u_i\{t\})[i] \cdot H(u_j\{t - 1\})[i]}}
\end{equation}

Here, $H(u)[i]$ is the $i^{th}$ bin of the histogram computed for the stixel $u$, and $d$ is the number of bins in the histogram. In our implementation, $d = 256$.

\subsubsection{Height difference}\label{ch:chapter04_01_03_01_05}

This metric is used to complement the other two metrics, as by its own is not enough to do a matching, but it is good enough to help in the decision, in case of very similar scores in two or more possible matches. $f_{height}$ is computed as follows:

\begin{equation}\label{eq:cp04_stixel_movement_height_cost}
f_{height}(u_i\{t\}, u_j\{t - 1\}) = 1 - |h(u_i\{t\} - h(u_j\{t - 1\})|
\end{equation}

, where $h(u)$ is the height of the stixel in column $u$. 

In our tests, we have tried different values for $\alpha_{SAD}$, $\alpha_{hist}$ and $\alpha_{height}$, obtaining the results shown in section \ref{ch:chapter04_02}, both in terms of performance and time. As said, the value obtained using the function $f_{cost}$ is used to weight the links between the nodes of a bipartite graph. This graph, whose nodes are the stixels at times $t$ and $t - 1$, is shown at figure \ref{fig:cp04_bipartite_graph}. In the top row, we can see the nodes (stixels) at current time, while in the lower row, previous stixels are represented. Costs of the match are represented by the edges, depicted in the image with the notation $\omega_{i,j}$.

\begin{figure}[h!]
\centering
\includegraphics{bipartite_graph}
\caption{Representation of the bipartite graph used for the matching process.}\label{fig:cp04_bipartite_graph}
\end{figure}

Over this graph, we solve the following minimization problem:

\comment{No estoy seguro de esta notaciÃ³n}
\begin{equation}\label{eq:cp04_match_minimization}
\mathcal{\hat{M}}=\underset{\mathcal{M}}{\arg\min} \underset{(i, j) \in \mathcal{M}}{\sum} \omega_{i,j}, 
~~~~\exists! (i, \cdot) \wedge \exists! (\cdot, j)
\end{equation}

This problem is solved using a $O(n \cdot m \cdot log(n))$ implementation of the Edmond's maximum weighted matching algorithm (\cite{edmonds1965paths}). The advantages of using this method instead the \ac{DP} implementation by \cite{gunyel2012stixels} is that we getter better times that the implementation available and, specially, we can ensure that each match is performed one-to-one. That is, in the \cite{gunyel2012stixels} implementation, the same stixel could be matched with more than one stixel from the other frame. This is not good, as we can have multiple paths from the same stixel, so it is difficult to know the trajectory it has followed in the past. In our implementation, we choose the matching set that maximizes the whole matching, ensuring that one stixel is matched with \emph{only} one stixel in the other frame.

\subsection{Obstacle-level tracking}\label{ch:chapter04_01_04}

In this stage, we perform the tracking at obstacle level, and not at stixel level, as described in the previous section. In the case of two-level tracking, we should have at this point the set of matchings $m(i,j) \in \mathcal{M}$, which relates each stixel at column $u_i$ in the frame $t$ with the stixel at column $u_j$ in the frame $t - 1$. Anyway, we do not really need to have these matches for this stage, and do the tracking directly at the obstacle level. This section comprises two steps: object detection (clustering) and object tracking.

\subsubsection{Clustering}\label{ch:chapter04_01_04_01}

From left to right, stixels are being evaluated using the algorithm shown at algorithm \ref{alg:cp04_clustering}.

\begin{algorithm}
\caption{Clustering algorithm}
\label{alg:cp04_clustering}
\begin{algorithmic}
\Function{Clustering}{$\mathcal{Q}\{t\}$}
  \State {$\mathcal{O} \gets \emptyset$}
  \State {$o \gets \emptyset$}
  \For {\textbf{each} stixel $q_i \in \mathcal{Q}$, from left to right}
    \If {$|depth(q_i) - depth(q_{i-1})| > \tau_{depth\_dist}$}
      \If {$|depth(q_i) - depth(q_{i-1})| > \tau_{depth\_dist}$}
	\State {$width(o) > \tau_{min\_width}$}
      \EndIf
      \State {$o \gets \emptyset$}
    \EndIf
    \State {$o \gets o \cup q_i$}
  \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

There, $\mathcal{Q}\{t\}$ is the set of stixels for the current frame $t$. From left to right, we start accumulating stixels, until the difference in depth between a certain pair of stixels is above a threshold $\tau_{depth\_dist}$, which is a user-defined free parameter. When this difference appears, we consider that we reached the right border of the obstacle, so it is added to the set $\mathcal{O}$, and a new obstacle is started. In case an obstacle is not wide enough, this obstacle is rejected. The reason for that is there are a stixel for each column, so there is a lot of noise that must be removed. This effect is easy to notice if we look at figure \ref{fig:cp04_stixels}.

At the end of the process, we will have got the set of obstacles $o_i \in \mathcal{O}$. Each obstacle has associated several parameters. One of them is the depth, which is computed as the minimal depth of all the associated stixels. In the left image of figure \ref{fig:cp04_clustering_aggregation}, the output obtained once this process has finished is shown.

\paragraph{Obstacle aggregation}\label{ch:chapter04_01_04_01_01}

In certain cases, due to a bad stixel detection, some stixels are located wrongly at a different depth from their real position. This effect happens, for example in cases like that shown at the left image of figure \ref{fig:cp04_clustering_aggregation}, in which a person in a fist plane opens his legs wide enough to show a big portion of the ground which is after him. This confuses the stixels detection algorithm, which thinks that the base of the obstacle is the central part of this person, and not his feet.

To solve this problem, the process described in algorithm \ref{alg:cp04_aggregation} is performed.

\begin{algorithm}
\caption{Aggregation algorithm}
\label{alg:cp04_aggregation}
\begin{algorithmic}
\Function{Aggregation}{$\mathcal{O}$}
  \State {$\mathcal{O'} \gets \emptyset$}
  \State {$o' \gets \emptyset$}
  \For {\textbf{each} object $o_i \in \mathcal{O}$, from left to right}
    \If {$|X(o_i) - X(o_{i-1})| > \tau_{lateral\_aggregation\_dist}$ \textbf{or}\\ \indent\indent\indent
	 $|Z(o_i) - Z(o_{i-1})| > \tau_{depth\_dist}$ \indent\indent~}

      \State {$\mathcal{O} \gets \mathcal{O} \cup o'$}
      \State {$o' \gets \emptyset$}
    \EndIf
    \State {$o' \gets o' \cup o$}
  \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

There, we test all obstacles previously detected, again from left to right. If the lateral distance (in world coordinates) is below a threshold $\tau_{lateral\_aggregation\_dist}$, we check again if the depth difference is below $\tau_{depth\_dist}$. If this condition is satisfied, both obstacles are joined together. In figure \ref{fig:cp04_clustering_aggregation}, an example of the output obtained from this process is shown. In the left image, the person in the first plane is divided into two different obstacles. After the aggregation process, this same person is assigned to a single obstacle. Depth of the obstacle is computed as the minimal depth of both original obstacles.

\begin{figure}[h!]
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{obstaclesBeforeAggregation}\label{fig:cp04_before_aggregation} &
\includegraphics[width=0.49\textwidth]{obstaclesAggregated}\label{fig:cp04_after_aggregation}
\end{tabular}
\caption{Comparison of the obstacles detected before and after the aggregation process.}\label{fig:cp04_clustering_aggregation}
\end{figure}

\paragraph{Obstacle filtering}\label{ch:chapter04_01_04_01_02}

If we look again at figure \ref{fig:cp04_clustering_aggregation}, we will notice that we are still detecting some fake obstacles. For instance, between the man with the red jacket and that with the dark suit, there are two of them. Next to the man in the second plane, also with a red jacket, there is another fake obstacle, and there is a last one in the right side of the image. Signs and poles are not considered fake obstacles, as they are elements to avoid, and we do not have a classification scheme able to distinguish the class of the obstacle being detected.

In order to distinguish real obstacles from fake obstacles, we register the images obtained between frames, so we can distinguish the movement between frames. This movement can be originated both by the movement of the obstacle itself (i.e. a person walking), or by the movement of the camera respect to the obstacles. This makes occluded areas to appear, which allows detecting the border of obstacles that are not moving, but which we want to avoid. This is the case, for example, of the pole in the right part of the image.

\begin{framed}

The best way to do such a registration is through a polar rectification process (\cite{pollefeys1999simple}). This non-linear polar rectification process is able to register the images along the time, making it easier to detect the motion. The basis of this method is to reparameterize the images by setting the coordinates of their pixels in terms of the epipoles of a stereo pair. In this process, a linear transformation has to be found for every wedge part of the image with vertex at the epipole. This allows rectifying the whole image for every possible epipolar configuration, while keeping the size of resulting image reasonably large. The algorithm is designed in such way that no pixel loss is guaranteed. Also the length of original epipolar lines is preserved. The resulting image is upper-bounded by a size of $(2 (W + H) \times \sqrt{W^2 + H^2})$, where $W$ is the image width and $H$ its height. 

In order to reduce matching ambiguity to a half of the epipolar line in a case of epipole inside the image, it uses the concept of oriented epipolar geometry. To do this, one point correspondence is needed in both views. With this information, just positive coordinates over the epipolar line will be taken into account. The transfer of corresponding epipolar lines is obtained from the expression:

\begin{equation}\label{eq:cp04_epipolar_lines}
l_{t - 1} \sim H^{-T}l_t\text{, or }l_t \sim H^T l_{t - 1}
\end{equation}

Here, $l'$ is the epipolar line at image at frame $t$, and $l$ the corresponding epipolar line at frame $t - 1$. $H$ is an homography for an arbitrary plane, which can be obtained from the fundamental matrix $F$ (\cite{luong1996fundamental}):

\begin{equation}\label{eq:cp04_homography}
H = [e_{t - 1}] \times F + e_{t - 1}^T a
\end{equation}

, where $a$ is a random vector for which $det(H) \neq 0$, so that $H$ is invertible. $e$ is the epipole.

The first step in the polar rectification process consists on determine the common region for both images. For that purpose, we first need to compute the epipoles for both images and the homography $H$. For that purpose, we need to know first the fundamental matrix $F$. The epipolar geometry is described by the following equation:

\begin{equation}\label{eq:cp04_fundamental_matrix}
m_{L,t - 1}^T \cdot F \cdot m_{L,t}= 0
\end{equation}

, where $m_{L,t - 1}$ and $m_{L,t}$ are homogenous representations of corresponding image points in the left image of the frames $t$ and $t - 1$, respectively. So we need to compute these matches. To do that, we compute the correspondences between image pairs in the following order: $I_{L, t} \rightarrow I_{R, t} \rightarrow I_{R, t - 1} \rightarrow I_{L, t - 1} \rightarrow I_{L, t}$, where $I_{\{L,R\},t}$ is the left ($L$) or right ($R$) image at frame $t$. From a initial set of features in $I_{L, t}$, we get the valid matches in $I_{R, t}$, and the cycle is completed until we reach $I_{L, t}$ again, keeping just the valid matches. A match is valid if satisfies the following rules:
\begin{itemize}
 \item At the end of the cycle, points obtained should be the same as those from which we started the process. If not, a wrong match was found in the way.
 \item Features in $I_{L, t}$ must be in the same row as $I_{R, t}$. As images are rectified, the vertical component should be the same. Same applies to $I_{L, t - 1}$ and $I_{R, t - 1}$.
 \item The 2D distance between features from frame $t$ and $t - 1$ should not be too big, as the time between frames is not too big.
\end{itemize}

The result of this matching process is represented at figure \ref{fig:cp04_polar_fund_matrix_computation}. There, each matching cycle is represented by the same random color.

\begin{center}
\includegraphics{fundamentalMatrixComputation}
\captionof{figure}{Looking for the common points in frames $t$ and $t - 1$.}\label{fig:cp04_polar_fund_matrix_computation}
\end{center}

With these matches, we solve the system described by equation \ref{eq:cp04_fundamental_matrix}, obtaining $F$. From $F$, it is possible to get also the homography $H$ and the epipoles $e_t$ and $e_{t-1}$. So we can start looking for the common region between the images. This common region will be determined by the extremal epipolar lines, which will be those that touch the outer image corners. We must deal with three possible cases depending on
whether the epipoles are located inside or outside the image. These three cases are shown at figure \ref{fig:cp04_polar_common_region}.

\begin{center}
\includegraphics{polar_common_region}
\captionof{figure}{The three possible cases depending on the position of the epipoles.}\label{fig:cp04_polar_common_region}
\end{center}

There, $I_i^j$ refer to the extremal lines for each image, where $i$ indicates if it is the current ($t$) or the previous ($t - 1$) image, and $j$ is the corner with which the line intersects (1 and 2 for the current image and 3 and 4 for the previous one), so $I_i^j = [e_i] \times c_j$. If the epipole is located inside the image we need to decide which half-epipolar lines, pointing in oposite direction, is the correct one. 

\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{epipolarExtremeLeft1}\label{fig:cp04_epipolarExtremeLeft1} &
\includegraphics[width=0.49\textwidth]{epipolarExtremeRight1}\label{fig:cp04_epipolarExtremeRight1} \\
\includegraphics[width=0.49\textwidth]{epipolarExtremeLeft2}\label{fig:cp04_epipolarExtremeLeft2} &
\includegraphics[width=0.49\textwidth]{epipolarExtremeRight2}\label{fig:cp04_epipolarExtremeRight2} \\
\end{tabular}
\captionof{figure}{Two examples of the external epipolar lines found for a case in which the epipole is inside the image for both images (top row); and out, also in both images (bottom row).}\label{fig:cp04_epipolarExtreme}
\end{center}


In image \ref{fig:cp04_epipolarExtreme}, two examples of the outer epipolar lines found are shown. From these, the common region is defined, which will be represented by the beginning and ending epipolar lines $I_i^B$ and $I_i^E$. If both epipoles are inside the image, an arbitrary epipolar line can be used. In that case, we can avoid boundary effects by adding a small overlap. That is, we use a little bit more from $360\textdegree$.

Then, starting from $I_i^B$, we start constructing the rectified image, line by line, until we reach the epipolar line $I_i^E$. This process is repeated for $i=t$ and $i=t-1$ so, at the end of it, we will have both images rectified. In these images, each of the rows will have a correspondence with a certain epipolar line. The distance between two consecutive epipolar lines is determined independently for each pair of epipolar lines so we avoid pixel compression. The benefits of doing such non-linear warping are that this allows getting the smallest image without information loss.

Using this method, we compute a transformation map that allows to know the correspondence of the pixel of each image in euclidean and in polar coordinates easily. By using such a map, we computed a pair of images as that shown in the left side of figure \ref{fig:cp04_polarRectification_example}. Red line demonstrates the proper alignment of them. 

\begin{center}
  \begin{tabular}{cc}
  \includegraphics[width=0.6\textwidth]{polarRectification}\label{fig:cp04_polarRectification} &
  \includegraphics[width=0.3\textwidth]{polarDiff}\label{fig:cp04_polarDiff}
  \end{tabular}
  \captionof{figure}{Example of a pair of frames rectified. Red line shows that the alignment is correct. On the right, the absolute difference of both rectified images is shown.}
  \label{fig:cp04_polarRectification_example}
\end{center}

\end{framed}

For the discrimination between real and fake obstacles, we align the current frame at time $t$ with another frame at time $t - k$. Using the aligned images, we get the pixel-wise absolute difference of both images, so we can get those pixels for which there is movement. This difference image is projected back to coordinates of the current image. Then, this difference is thresholded and binarized, rejecting the small differences produced by noise. Result of this process is shown at the top of figure \ref{fig:cp04_obstacle_filtering}. Then, for each obstacle, we get the associated \ac{ROI}. For each \ac{ROI}, we reject the top half of it, so we just look for movement in the area of the obstacle that is touching the ground. There are two reasons for that: the first one is that obstacles that move over the ground tend to have a higher movement in the lower part (for instance, legs or wheel movements). In the case of first plane static obstacles, the movement due to the camera change is more or less similar in the upper and lower half, so we are not loosing information. The second reason is that the stixels reconstruction assumes a planar ground in front of the camera, it is a fast way to discriminate bad correspondences.

\begin{figure}[h!]
  \begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thresholdedPolar}
    \end{minipage}\hfill~
  \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|}
      \hline
      0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
      \hline
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi0} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi1} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi2} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi3} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi4} & 
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi5} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi6} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi7} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi8} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi9} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi10} \\
      \hline
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ0}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ1}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ2}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ3}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ4}} & 
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ5}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ6}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ7}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ8}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ9}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ10}} \\
      \hline
    \end{tabular}
  \end{minipage}\hfill
  \caption{Object filtering process. In the top, the binarized motion image is shown ($k=0.2$). In the bottom, the occupancy maps generated for each candidate obstacle.}\label{fig:cp04_obstacle_filtering}
\end{figure}

Each region is transformed to real world coordinates and project there the points resulting after the thresholding process. There, the region is dividided in cells (in our tests, of $10\times10\,cm$). For each cell, if there is at least one point falling inside it in the plane $XY$, the cell is marked as occupied. The occupancy grids obtained, together with their associated \ac{ROI} in the motion image, are shown in the table in figure \ref{fig:cp04_obstacle_filtering}. \acs{ROI} are in the first row, and occupancy maps in the second. It is easy to see that real obstacles, like for example number 5 or number 7, present a higher density if compared with, for example, the obstacle number 4. Also, if we look at obstacle 2, a man in a black suit (which makes the motion detection a little bit more complicated through absolute differencing due to the low color values), is properly detected. In order to ensure the detection of this kind of obstacles, it is important to choose a value for $k$ big enough. In our tests, we decided to use $k = 0.2\,s$, so we are sure that differences are appreciable, but being conservative at the same time. At the end of the process, we decide if the obstacle is rejected or not through the following equation:

\begin{equation}\label{eq:cp04_fake_obstacles}
  fake(o) = 
  \begin{align*}
    \begin{cases}
      true & \text{if } {{count(G_o, true)} \over {count(G_o, true) + count(G_o, false)}} > \tau_{occ} \\
      false & otherwise
    \end{cases}
  \end{align*}
\end{equation}

There, $count(G_o, j)$ counts the number of occupied ($j=true$) or free ($j=false$) cells in the occupancy grid corresponding to the object $o$, $G_o$. $\tau_{occ}$ is a parameter. We also check that the width of each obstacle in real world coordinates is inside the limits. We can see the results obtained for this process again in figure \ref{fig:cp04_obstacle_filtering}. Rejected obstacles are highlighted in red, while the accepted are represented in green.

\subsubsection{Tracking}\label{ch:chapter04_01_04_02}

Once we have the obstacles detected and filtered, we propose two approaches for the object tracking. The first one takes advantage of the process described in section \ref{ch:chapter04_01_03_01}. From the initial matching performed at stixel level, we try to maximize the number of matches between obstacles. The second one tries to do the matching directly. As there is not a big difference between frames, we can use template matching techniques in order to do the tracking. As we will see in section \ref{ch:chapter04_02_03}, each of these techniques has its own advantages. The first one presents a better recall along the frames; however, the second is faster, with a recall that, despite of being not as good as that for the two-level tracking scheme, is still good.

\paragraph{Two-level tracking case}\label{ch:chapter04_01_04_02_01}

As said, the two-level tracking takes advantage from the stixels matched using the method in \ref{ch:chapter04_01_03_01}, together with the obstacles found using the method described in the last section. Again, we consider the tracking problem as a pair matching process that is repeated along the time. Based on that idea, we define the correspondence matrix $C_{|\mathcal{O}\{t\}| \times |\mathcal{O}\{t - 1\}|}$, which counts the number of correspondences obtained at stixel level between the stixels at the current frame and the previous one. This process is described in the algorithm \ref{alg:cp04_two_level_tracking}.

\begin{algorithm}
\caption{Two-level tracking algorithm}
\label{alg:cp04_two_level_tracking}
\begin{algorithmic}
\Function{Tracking}{$\mathcal{O}\{t\}$, $\mathcal{O}\{t - 1\}$}
  \State {$C_{|\mathcal{O}\{t\}| \times |\mathcal{O}\{t - 1\}|} \gets 0$}
  \For {\textbf{each} object $o\{t\} \in \mathcal{O}\{t\}$}
    \For {\textbf{each} stixel $q\{t\} \in o$}
      \State {Find correspondence $q\{t - 1\}$ for $q\{t\}$}
      \State {Find the object $o\{t - 1\} \in \mathcal{O}\{t - 1\}$ associated to $q\{t - 1\}$}
      \If {$o\{t - 1\}$ found \textbf{and} $\|o\{t\} - o\{t - 1\}\| < \tau_{max\_obst\_dist}$}
	\State {$C(o\{t\}, o\{t - 1\}) \gets C(o\{t\}, o\{t - 1\}) + 1$}
      \EndIf
    \EndFor
  \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

Two objects can be associated between frames if there is at least one stixel correspondence and they are close enough, as we assume the movement between frames should not be too big as the frame rate is high enough. With this cost matrix, we solve the following maximization problem:

\begin{equation}\label{eq:cp04_two_level_maximization}
\mathcal{\hat{C}}=\underset{\mathcal{C}}{\arg\max} \underset{(i, j) \in \mathcal{M}}{\sum} C(i,j),
~~~~\exists! (i, \cdot) \wedge \exists! (\cdot, j)
\end{equation}

After this process, we will have the set of pairs $\mathcal{\hat{C}}$. In our implementation, we maintain an internal structure that associates each track to each obstacle in the last tracked frame. These tracks are extended in order to include the new objects, which will be used as the new indexes of the trajectories list. In figure \ref{fig:cp04_tracking_examples_two_level}, we can see several examples of the results obtained using this tracking method. For more information about the performance obtained, please check the results in section \ref{ch:chapter04_02} \notsure{, or check the videos available at XXX}.

\begin{figure}[h!]
    \centering
    \begin{tabular}{ ccccc}
      \includegraphics[width=0.45\textwidth]{sequenceTwoLevel/twolevel30}\label{fig:cp04_two_level_example_15} &
      \includegraphics[width=0.45\textwidth]{sequenceTwoLevel/twolevel320}\label{fig:cp04_two_level_example_126} &
    \end{tabular}
  \caption{Some tracking results using the two-level based tracking.}\label{fig:cp04_tracking_examples_two_level}
\end{figure}

\paragraph{Object tracking case}\label{ch:chapter04_01_04_02_02}

The object tracking scheme is quite similar. However, in this case the cost matrix is not generated by counting the number of stixels associated to each obstacle, as we do not have this information (remember that for this approach, we do not apply the process described in section \ref{ch:chapter04_01_03_01}). Instead of that, we use the histograms difference for each pair of obstacles, in a process similar to that described in section \ref{ch:chapter04_01_03_01_02}, so the cost matrix in this case is defined as:

\begin{equation}\label{eq:cp04_object_matching_histograms_cost}
C(o\{t\}, o\{t - 1\}) = 1 - \left ( 2 \cdot \sqrt { 1 - \underset{i=1}{\overset{d}{\sum}}\sqrt{H(o\{t\})[i] \cdot H(o\{t - 1\})[i]}} \right )
\end{equation}

Then, the tracking problem is the same as explained for the two-level tracking case: we solve the maximization problem defined in \ref{eq:cp04_two_level_maximization}, and then the tracks are updated with the results of the matching. In figure \ref{fig:cp04_tracking_examples_object}, some other examples of the output obtained after applying this process are shown.

\begin{figure}[h!]
    \centering
    \begin{tabular}{ccccc}
      \includegraphics[width=0.45\textwidth]{sequenceObstacle/obstacle15}\label{fig:cp04_object_level_example_30} &
      \includegraphics[width=0.45\textwidth]{sequenceObstacle/obstacle126}\label{fig:cp04_object_level_example_320}
    \end{tabular}
  \caption{Some tracking results using the object based tracking.}\label{fig:cp04_tracking_examples_object}
\end{figure}

\section{Results}\label{ch:chapter04_02}

In this section, we will show some evaluation results obtained after the evaluation of the method described in this chapter. Evaluations have been focused into four different scopes: 

\begin{itemize}
 \item Quality of the clustering process.
 \item Accuracy of the depth obtained for the computed stixels compared to object level results.
 \item Recall of the obtained tracks related to several conditions.
 \item Computation time.
\end{itemize}

In order to compare our results with those obtained by \cite{gunyel2012stixels} and \cite{benenson2011stixels}, we have used the \emph{Bahnhof} sequence (\cite{ess2009robust}), which contains about 7400 obstacle annotations with height $\geq 40\,px$ on 999 stereo pairs, with an image resolution of $640 \times 480$ pixels and a frame rate of about 15 frames per second. All tests described in this section have been performed over this sequence.

\subsection{Clustering}\label{ch:chapter04_02_01}

In this test, we want to know if the obstacle detection method described in section \ref{ch:chapter04_01_04_01} is good enough. For that, we compared our detections with the real obstacles appearing in each current frame. In these tests, we have executed the method with and without performing the filtering process described in section \ref{ch:chapter04_01_04_01_02}, obtaining the results shown in figure \ref{fig:cp04_detection_rate}.

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{detectionRate}
\caption{Obstacles detection rate achieved using our clustering method.}\label{fig:cp04_detection_rate}
\end{figure}

There, we compare each recall value with the number of frames in the sequence falling below it. That is, the faster the plot grows, the better the results are. There, we can see that, for a recall value of 90\%, just a 40\% fall below this value for the case in which we are filtering the obstacles. Situation is different when objects are not filtered. For the same recall value, a 60\% of the frames in the sequence are below this value.

In figure \ref{fig:cp04_clustering_comparison}, we can see the projected 3d output of the detected stixels. In the left image, the original stixels are shown. As we can see, there is a lot of noise, specially between obstacles. We can see also that a lot of free areas are detected. In the right image we show just the stixels falling inside an obstacle, and with their depths restored.

\begin{figure}[h!]
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{stixelsDetection}\label{fig:cp04_stixels_detection} &
\includegraphics[width=0.49\textwidth]{obstacleDetection}\label{fig:cp04_obstacle_detection}
\end{tabular}
\caption{Comparison of the stixels obtained trough the method of \cite{benenson2012pedestrian} and the clustering performed in our method.}\label{fig:cp04_clustering_comparison}
\end{figure}

\subsection{Stixel accuracy}\label{ch:chapter04_02_02}

In this section, we want to show the accuracy found in the depth computation for the stixels regarding to our reconstruction based just on the obstacles. There, we compare the disparity error regarding to a \ac{ELAS} reconstructed disparity map wit the percentage of frames below this error. The red line represents the stixels error, which grows faster than the error shown by the clustered obstacles. If we look at the $x-coordinates$, we can see that approximately a 95\% of the images have a disparity error below a 10\%, while this error is just found for a 60\% of the images when the reconstruction is made buy just the original stixel computation.

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{disparity}
\caption{Difference in disparity achieved for both our clustering method and the initial stixel reconstruction.}\label{fig:cp04_disparity_comparison}
\end{figure}

In figure \ref{fig:cp04_reconstruction}, we show the disparity maps presented by \ac{ELAS}, the original stixels from \cite{benenson2011stixels} and our clustered obstacles. These are represented by a color scale, which is shown in the right side of the images. In this scale, the lower disparities (further) are represented in red, while the highest disparities are represented in blue.

\begin{figure*}[h!]
        \centering
        \begin{subfigure}[b]{0.25\textwidth}
	  \begin{tabular}{c}
	    \includegraphics[width=\textwidth]{elas}
	  \end{tabular}
	  \caption{ELAS.}\label{fig:cp04_reconstruction_elas}
        \end{subfigure}% 
        ~
        \begin{subfigure}[b]{0.25\textwidth}
	  \begin{tabular}{c}
	    \includegraphics[width=\textwidth]{stixels}
	  \end{tabular}
	  \caption{Stixels.}\label{fig:cp04_reconstruction_stixels}
        \end{subfigure}%       
        ~
        \begin{subfigure}[b]{0.25\textwidth}
	  \begin{tabular}{c}
	    \includegraphics[width=\textwidth]{objects}
	  \end{tabular}
	  \caption{Object clustering.}\label{fig:cp04_reconstruction_objects}
        \end{subfigure}%    
        ~
        \begin{subfigure}[b]{0.25\textwidth}
	  \centering
	  \begin{tabular}{c}
	    \includegraphics[height=0.375\figuresheight]{colorscale_jet}
	  \end{tabular}
	  \caption*{}\label{fig:cp04_reconstruction_colorscale}
        \end{subfigure}% 
        \caption{Comparison of the disparities obtained for \ac{ELAS}, stixels and the reconstructed objects.}\label{fig:cp04_reconstruction}
\end{figure*}

\subsection{Tracking}\label{ch:chapter04_02_03}

In this section, the results obtained in our tracking evaluation tests are shown in terms of the recall measured related to two different criteria: the behavior of the tracking after a few frames (that is, the track length achievable with a certain level of confidence); and the behavior when the time between frames is increased. 

In our tests, we have tried with different configurations attending to the matching metrics and the method used. From the whole set of tests, we have selected the most representative, which are shown in table \ref{table:cp04_configurations_tested}.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
  \hline
 \multirow{2}{*}{Name} & \multicolumn{3}{ c| }{Cost factors} & \multirow{2}{*}{Tracking Method} \\ \cline{2-4}
 & $\alpha_{SAD}$ & $\alpha_{hist}$ & $\alpha_{height}$ &  \\
 \hline
 Conf. 1 & 1 & 0 & 0 & \cite{gunyel2012stixels} \\
 Conf. 2 & 0.5 & 0 & 0.5 & \cite{gunyel2012stixels} \\
 \hline
 Conf. 3 & 1 & 0 & 0 & Two-level tracking \\
 Conf. 4 & 0.5 & 0 & 0.5 & Two-level tracking \\
 Conf. 5 & 0 & 1 & 0 & Two-level tracking \\
 Conf. 6 & 0 & 0.5 & 0.5 & Two-level tracking \\
 \hline
 Conf. 7 & 0 & 0 & 0 & Object tracking \\
 \hline
\end{tabular}
\end{center}
\caption{Configurations for which the evaluation results are shown.}\label{table:cp04_configurations_tested}
\end{table}

Two of these configurations are from the implementation of the method by \cite{gunyel2012stixels}, available at \url{https://bitbucket.org/rodrigob/doppia}, with just the \ac{SAD} cost, and the final configuration described in their paper, in which $\alpha_{SAD} = 0.5$ and $\alpha_{height} = 0.5$, in order to know the real contribution of factor $\alpha_{height}$. Same process is done for the two-level tracking method, in which we evaluate also the contribution of factor $\alpha_{hist}$. Finally, we do a evaluation of the results obtained by using the object based tracking, without doing a previous stixel level tracking. As this level is not needed, $\alpha_{SAD} = \alpha_{hist} = \alpha_{height} = 0$.

\subsubsection{Performance along the sequence}\label{ch:chapter04_02_03_01}

For the evaluation of the tracking capabilities of the method, we followed the same strategy as \cite{gunyel2012stixels}: we used annotated obstacle bounding boxes provided as ground truth with the evaluated sequence. Starting from ground truth annotations at a certain frame, each evaluated configuration is used to predict the bounding box positions up to $\Delta$ frames in the future. For each frame, we evaluate the recall using the standard intersection over union metric. By running this evaluation starting from every frame in a video sequence we obtain the \emph{recall vs. $\Delta$ frames} curve shown at figure \ref{fig:cp04_recall_vs_delta_frames}, that can be used to compare the configurations. 

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{recall_vs_delta_frames}
\caption{Comparison of the recall obtained for the different configurations.}\label{fig:cp04_recall_vs_delta_frames}
\end{figure}

In this chart, we can see that the \cite{gunyel2012stixels} method falls quite fast, with a recall below 50\% just after 5 frames have passed. Also, we notice that the contribution of $\alpha_{height}$ is not clear. About the two-level tracking methods, results are better, specially those in which $\alpha_{hist} \neq 0$. There are many reasons for that. Respect with the method of \cite{gunyel2012stixels}, the use of obstacles for the tracking instead of just the stixels filters a lot of noise, making the tracking more reliable. This is confirmed if we look at figure \ref{fig:cp04_tracking_examples}, where we can see the tracking performed by \emph{Configuration 1}, \emph{Configuration 5} and \emph{Configuration 7}. If we look at the first two images, we can see that the trajectories obtained for the \emph{Configuration 5} are longer, and smoother. About the difference between using $\alpha_{hist}$ or $\alpha_{SAD}$, the histograms being used are normalized just before the matching, while the sum of absolute differences is done pixel by pixel, without considering illumination changes.

Finally, object based tracking shows good results for the first frames, but it falls a little bit faster than the two-level based tracking methods. We think the most likely reason for that is that the two-level tracking is more tolerant to the clustering errors. For example, if in one frame we consider as part of the obstacle a relatively big fraction of the background, the aspect of the histogram will change, so the matching score could be small. Looking again at figure \ref{fig:cp04_tracking_examples}, we can see that the quality of the tracks both in configurations 5 and 7 is comparable, but the length of the former is bigger.

\begin{figure*}[h!]
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
	  \begin{tabular}{c}
	    \includegraphics[width=\textwidth]{trackingConf1}
	  \end{tabular}
	  \caption{Conf. 1.}\label{fig:cp04_tracking_example_conf_1}
        \end{subfigure}% 
        ~
        \begin{subfigure}[b]{0.33\textwidth}
	  \begin{tabular}{c}
	    \includegraphics[width=\textwidth]{trackingConf5}
	  \end{tabular}
	  \caption{Conf. 5.}\label{fig:cp04_tracking_example_conf_5}
        \end{subfigure}%       
        ~
        \begin{subfigure}[b]{0.33\textwidth}
	  \begin{tabular}{c}
	    \includegraphics[width=\textwidth]{trackingConf7}
	  \end{tabular}
	  \caption{Conf. 7.}\label{fig:cp04_tracking_example_conf_7}
        \end{subfigure}%
        \caption{Example of the tracking obtained for the different configurations, at stixel level.}\label{fig:cp04_tracking_examples}
\end{figure*}

\subsubsection{Performance at different frame increments}\label{ch:chapter04_02_03_02}

We also wanted to know which of the methods was more tolerant to low frame rate sequences, as one of the assumptions for the tracking methods is that the temporal difference between frames is not too big. Based on this, we evaluated the relation existing between the recall, $\Delta$ frames and the time step between frames. So we repeated the tests, but this time increasing $k$ frames each time, with $k=0.06\dots1.2\,s$ (Corresponding to 1 up to 20 frames at $15\,Hz$). Then, we evaluated the variation in the results. First, we did a few preliminary tests with just the first 200 frames in which we wanted to know the variation in the recall related to the time step. Results of this tests are represented in the chart shown at figure \ref{fig:cp04_recall_vs_step}. In this chart, we show the tracking values presented by the different configurations with just one frame increment at the time step shown in the $x$ axis. There, we can detect four different profiles, which are again related to the configurations 1-2, 3-4; 5-6, and 7. As before, we observe that the contribution offered by the $\alpha_{height}$ factor is negligible. We also observe that the most tolerant of all the configurations is number 7, as it was to be expected. This configuration does the tracking just at object level, so it is able to deal with changes slightly bigger than those with which the stixel level tracking is able to work properly. However, there is a surprise, as the tracking based in just the stixel level presents better values. The reason for that is that tracking using this method gets slightly better results in the first frames, but the recall falls faster along the frames.

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{recall_vs_step}
\caption{Behavior of the recall obtained for the different configurations at different frame increments.}\label{fig:cp04_recall_vs_step}
\end{figure}

This effect is better seen in figure \ref{fig:cp04_recall_vs_delta_frames_vs_step}, where we compare the three involved values (recall, $\Delta frames$ and $\Delta time$ for the configurations 1, 3, 5 and 7. There, we can see that when $\Delta frames \approx 0$, the pattern shown in the previous figure is repeated. However, when $\Delta frames$ starts growing we can see that configurations 3 and 5 do not fall as fast as configuration 1, which confirms the tests previously done. Configuration 7 remains higher with respect to the rest of configurations.

\begin{figure}[h!]
\centering
\includegraphics[trim=80 90 140 90,clip]{recall_vs_delta_frames_vs_step_28b_1_16b}
\caption{Comparison of the tracking capabilities at different frame increments for configurations \emph{Conf. 1}, \emph{Conf. 5} and \emph{Conf. 7}}\label{fig:cp04_recall_vs_delta_frames_vs_step}
\end{figure}

\subsection{Computation time}\label{ch:chapter04_02_04}

In figure \ref{fig:cp04_times_average}, we can see that, as expected, the faster of the methods is that for the \emph{Configuration 7}, as just object comparison is performed. This is quite faster, as just a few obstacles are compared per frame, against the 640 by 640 comparisons that can be achieved at stixel level in the worst case. From the rest, we see that the graph based methods are in average slightly faster than those based on dynamic programming. However, the time variance obtained when using the sum of absolute differences as measure is bigger. We do not have an explanation for that, probably if the situation is not quite discriminative, the graph based method needs more time for computing the matches.

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{times_average}
\caption{Times obtained for each configuration.}\label{fig:cp04_times_average}
\end{figure}

\section{Summary}\label{ch:chapter04_07}

In this chapter, we have seen a different solution for the object tracking for driver assistance applications, based on the stixel world by \cite{badino2009stixel}. Our work extends the work presented by \cite{gunyel2012stixels}, improving the results obtained by them. The use of a two-level based tracking gives robustness to the stixel tracking and is able to improve the reconstruction by joining stixels that were not in the same plane due to a bad reconstruction. We demonstrate that using the Hellinger distance between histograms give better results than the pure Sum of Absolute Differences, and that the use of the height as matching metric is negligible.
We also saw another tracking method in which the tracking is performed at obstacle level. In this case, results were not so good, but in certain applications can be useful due to a high performance time and a low decay in the recall when the frame rate is low.
In the future, we want to try to improve the way in which stixels are computed. We think that using the polar rectification for combining tracking and reconstruction could give good results. Also, the use of a measure of the goodness of the tracks at stixel level should help to improve the results obtained for the clustering process.
In the next chapter, we will see another obstacle tracking method in which 3D dense reconstruction is used as input.

