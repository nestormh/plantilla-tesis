%%
%%  chapter05.tex - Obstacle Detection and Planning for Autonomous Vehicles based on Computer Vision Techniques
%%
%%  Copyright 2014 Néstor Morales <nestor@isaatc.ull.es>
%%
%%  This work is licensed under a Creative Commons Attribution 4.0 International License.
%%

\graphicspath{{./images/chapter04/bmps/}{./images/chapter04/vects/}{./images/chapter04/}}

\chapter{Stixel World}\label{ch:chapter04}

In the previous chapter, we saw a set of algorithms and configurations able to construct a disparity map from a pair of stereo images. These methods are useful for the reconstruction of the environment. However, this reconstruction is dense, which leads to an intensive usage of the computer resources. In order to solve this problem, some authors propose approaches that try to minimize the area of the image being processed by doing a simpler reconstruction than that done with dense 3D reconstruction algorithms. In this sense, \cite{badino2009stixel} proposed to represent the world by a set of rectangular sticks named \emph{stixels} (from \emph{stick} and \emph{pixel}). Each stixel is defined by its 3D position relative to the camera and stands vertically on the ground, having a certain height.
This compact but flexible representation of the world can be used as the common basis for the scene understanding tasks of driver assistance and autonomous systems. The main advantages of using such an approach are listed next:
\begin{itemize}
 \item \emph{Compact}. Significant reduction of the data volume.
 \item \emph{Complete}. Information of interest is preserved.
 \item \emph{Stable}. Small changes of the underlying data must not cause rapid changes within the representation.
 \item \emph{Robust}. Outliers must have minimal or no impact on the resulting representation.
\end{itemize}

The method described in this chapter is based in the fast stixels implementation described in \cite{benenson2012pedestrian}. One of the main advantages of this implementation is that there is no need of obtaining a previous depth map. This fact is also the main reason for which we did not use the original implementation from \cite{badino2009stixel} or that from \cite{pfeiffer2010efficient}. Based on the output of this method, we do the tracking of the stixels following a bipartite-graph method based on a given cost metric. In this sense, we have tested several cost metrics, which are described in the following sections.

Our tracking method is based on the contribution by \cite{gunyel2012stixels}. There, movement is just computed for the areas which are covered by stixels. It has been demonstrated that stixels are good enough to do a representation of the surroundings of a vehicle, and that they have a detail level enough for the movement detection. Traditionally, this movement detection is carried out through the computation of the optical flow between two frames, which is computationally expensive. In this chapter, we will see how this movement can be computed also using the stixel world representation, by extending the method of \cite{gunyel2012stixels}. For that, we have followed two different approaches, each with its own benefits and drawbacks. The first tries to cluster the stixels in objects, so the tracking is performed at object level; the second joins the stixel tracking with the object tracking in a two level tracking schema.

Our contribution in this work is summarized next:
\begin{enumerate}
 \item Improvement of the reconstruction obtained by the stixels. The free space computation without the use of a disparity map have some drawbacks. Some of them are related with the fact that the precision in the reconstruction of the obstacles is not good enough. The object reconstruction detection scheme proposed in this chapter allows the correction of the stixel depths and the removal of the fake obstacles, as described in sections \ref{ch:chapter04_02_01} and \ref{ch:chapter04_02_02}.
 \item Improvement of the results obtained by the method described in \cite{gunyel2012stixels}, in both object and two-level tracking approaches. We modified the method by using a graph-based approach instead of a \ac{DP} based method, as they did. 
 \item Different cost metrics for the tracking have been tested, with promising results. 
 \item Faster tracking. As we will see in section \ref{ch:chapter04_02_04}, the speed achieved with our implementation is better, specially in the object based tracking. The two-level tracking is also a little bit faster, thanks to the usage of a bipartite graph based method for the matching of the stixels between frames.
 \item \notsure{Robustness after a big change between images (for example, in presence of a low framerate).}
\end{enumerate}

In the following section, each of the steps of the method are detailed. At the end of the chapter some results are shown in order to evaluate the performance of the described method.

\section{The Method}\label{ch:chapter04_01}

The method follows a pipeline as that described in figure \ref{fig:cp04_pipeline}. First, from a given pair of images, the free space in front of the vehicle is computed in order to estimate the ground plane. From the extracted ground plane, stixels are obtained. Then, a two-level tracking approach is started. The first level is that corresponding to the tracking of the stixels. This stage is inspired by the work described in \cite{gunyel2012stixels}. The set of stixels computed for the current frame are compared with those from the previous frame. Based on this comparison, some of them are matched. At the same time, we cluster stixels based on their position once projected in the 3d world. Using the clusters and the tracked stixels, we start a new tracking step in which the tracking is performed at object level. At the end of this step, we will know the obstacles in the scene and their velocities, as well as an historic of the path followed by them that could be useful for future movement estimation.

We also propose another solution in which just the object level tracking is performed. In this case, instead of doing a comparison at stixel level, we compare each obstacle to the obstacles detected in the previous frame, so the stixel level tracking is not needed anymore. In section \ref{ch:chapter04_02}, we will see the advantages and drawbacks of using one or another approach.

\begin{figure}[h!]
  \centering
  \includegraphics{pipeline.jpg}
  \caption{Pipeline of the stixels tracking method described in this chapter.}\label{fig:cp04_pipeline}
\end{figure}

\subsection{Free space computation}\label{ch:chapter04_01_01}

In this section and in the following, we will describe the stixel extraction method, which is similar to that described in \cite{benenson2012pedestrian}. This method works under the following assumptions:

\begin{itemize}
 \item The input of the algorithm is a calibrated stereo image pair.
 \item We use the Lambertian surface assumption.
 \item The ground is planar, at least at local level.
 \item The objects of interest are mainly vertical, and have a limited height range.
 \item Stereo rig has a negligible roll with respect to the ground plane.
\end{itemize}

In this section, we introduce the first step needed for the stixels estimation process, which is the ground plane estimation stage. This ground plane is estimated using the evidences collected in the \emph{v-disparity} domain. Instead of computing and projecting a depthmap obtained, for example, one of the methods evaluated in chapter \todoref{XXX}, the evidence is collected directly from matching the rows from the left and right images at different disparities, obtaining a cube $(U, V, D)$ in which each cell represents the cost of matching the pixel at $p_i(u_i, v_i) \in I_L$ with that at $q_i(u_i + d_i, v_i) \in I_R$. Here, $p_i$ and $q_i$ are pixels from the left ($I_L$) and right ($I_R$) images, respectively; and $u_i$, $v_i$ and $d_i$ are the different possible column, row and disparity values in the cube. In figure \ref{fig:cp04_freespace}, a graphical description of the obtained cube is represented.

\begin{figure}[h!]
  \centering
  \includegraphics{freespace}
  \caption{Free space computation process.}\label{fig:cp04_freespace}
\end{figure}

For each row $v_i$, the disparity with the lowest cost is extracted for each row, as depicted in figure \ref{XXX}. From these values, a robust line fitting is used in order to find the ground plane, which is represented by the following expression:

\begin{equation}\label{eq:cp04_ground_plane_function}
  d_i = f_{ground}(v_i)
\end{equation}

Using this equation, we can know the disparity associated to each row at ground level, which will we use later for the stixels computation. For optimization purposes, instead of collecting the evidences for each row below the horizon, just one out of each $N$ rows is computed. 

\subsection{Stixels extraction}\label{ch:chapter04_01_02}

With the ground plane, we can start detecting the stixels in the images. The way in which this is done is by dividing the image in multiple row bands $b_i$. Inside each band $b_i$, and for each column $u_i$, the pixel with the biggest horizontal gradient is selected. This reduces the computational cost while the possibilities of finding the border of the object accurately are increased. Also, one of the advantages of using bands and not the rows directly is that, in presence of a confusing horizontal line, the effect is compensated by the band height.

From \ref{fig:cp04_freespace}, we obtain:

\begin{equation}\label{eq:cp04_ground_plane_function_by_band}
  d(q_j, b_i) = f_{ground}(v(q_j, b_i))
\end{equation}

Here, $q_j$ makes reference to the stixel $j$, where the total number of stixels is the total number of columns in the image divided by a parameterized stixel width $\tau_{stixel\_width}$. That is, $j=1 \dots \tau_{stixel\_width}$. In general, interest objects are wider than one column, so computing an evidence for each column in the image is redundant. Each stixel is located in the column $u(q_j) = j \cdot \tau_{stixel\_width}$. $v(q_j, b_i)$ is a certain row inside the stixel $q_j$ in the band $b_i$. 

At this point, the goal is to localize the optimal band for each stixel. This is based on the following expression:

\begin{equation}\label{eq:cp04_stixel_band_cost}
  b^*_s (q) = \underset{b(q)}{\arg\min} \underset{q}{\sum}c_s(q, b(q)) + \underset{q_a, q_b}{\sum}s_s(v(q_a, b(q_a)), v(q_b, b(q_b)))
\end{equation}

Here, we can think on $c_s$ as the data term, while $s_s$ is the smooth term. $q_a$ and $q_b$ are neighbors. That is, $|q_a - q_b| = 1$

\paragraph{Data term}\label{ch:chapter04_01_02_01}

By computing the cost $c_s$, we know the likelihood of the presence of an stixel $q$ at the row band $b$. The lower the cost is, the more possible that there is a stixel. This cost is computed as follows:

\begin{equation}\label{eq:cp04_stixel_band_cost_data_term}
  c_s(q, b) = c_o (u(q), d(q, b)) + c_g(u(q), d(q,b))
\end{equation}

This equation is composed by two terms:
\begin{itemize}
 \item \emph{Object cost($c_o$)}. It represents the cost of the presence of a vertical object. It just sums the evidence along the vertical column, using the expected height of the object, which is projected on the image using the distance given by the ground plane.
 \item \emph{Ground cost($c_g$)}. It represents the cost of a supporting ground being present, and sums the evidence along the ground plane.
\end{itemize}

\paragraph{Smooth term}\label{ch:chapter04_01_02_02}

The smooth term ($s_s$) forces to respect the left-right occlusion restrictions and promotes ground object boundaries with few jumps.

\begin{equation}\label{eq:cp04_stixel_band_cost_smooth_term}
  s_s(v_a, v_b) = 
  \begin{align*}
    \begin{cases}
    \infty & \text{if } d(v_a) < d(v_b) - 1 \\
    c_o(u_a, d(v_a)) & \text{if } d(v_a) \approx d(v_b) - 1 \\
    - \omega \cdot c_o(u_a, d(v_a)) & \text{if } q_a = q_b \\
    0 & \text{if } d(v_a) > d(v_b) - 1
    \end{cases}
  \end{align*}
\end{equation}

Here, $\omega$ is a free parameter, chosen by the user, which promotes boundaries with a few jumps. At this point, we will have extracted the stixels, obtaining a result similar to that shown at figure \ref{fig:cp04_stixels}, in which the stixels (in green) are superimposed to the left image from which they were extracted. For more information about the process described in sections \ref{ch:chapter04_01_01} and \ref{ch:chapter04_01_02}, please refer to \cite{benenson2012fast}.

\begin{figure}[h!]
  \centering
  \includegraphics{stixels_over_original}
  \caption{Stixels superimposed to the frame from which they were extracted.}\label{fig:cp04_stixels}
\end{figure}

\subsection{Tracking}\label{ch:chapter04_01_03}

At this point, we are ready to start tracking the stixels. As said before this tracking has been performed using two different approaches: one consists on two different tracking levels from the first one tracks the stixels independently (that is, each stixel at the previous frame $t - 1$ is matched with another - or none - at the current frame $t$). So it becomes a matching problem in which we try to find the minimal cost matching between frames. In \cite{gunyel2012stixels}, this minimal matching was done using \ac{DP}. In our implementation we use a bipartite graph matching based method. In a second level, stixels are clustered into objects, which are matched based on the previous stixels tracking.

In the other approach, just the second level is performed, tracking obstacles based on one of the cost metrics, without considering the stixels included in the objects (except for the clustering process). In the next sections, the tracking process is described. Please note that the process described in section \ref{ch:chapter04_01_03_01} is performed only for the two-level approach, while the process in \label{ch:chapter04_01_04} is common to both approaches, except from the differences explicitly indicated in that section.

For this stage, we made some assumptions:
\begin{itemize}
 \item First, we assume that all stixels have been properly estimated.
 \item The maximal speed of the objects is limited, so we limit the search range between stixels to a certain threshold. This range depends on the distance of the stixel at the current frame and the frame rate. As there is just one stixel per column, we can limit also the matching process to a search into the $u$ direction.
 \item There is not a big temporal difference between two consecutive frames. That means that the same stixel at time $t$ and $t - 1$ should be similar. Also their height in meters should be similar. \notsure{As shown in section \ref{ch:chapter04_02_03_02}, this temporary consistence is more or less restrictive depending on the approach finally used for the tracking process.}
\end{itemize}

\subsubsection{Stixels-level tracking}\label{ch:chapter04_01_03_01}

In this stage, the tracking is performed at stixel level. As we just try to match each stixel at column $q_i\{t\}$ with another in the previous frame $t - 1$, we can think on this process as a matching problem. The way in which we have solved this is through a bipartite graph, in which nodes are the stixels at frame $t$ and $t - 1$, and the edges are associated to a certain movement cost $c_m$, which is represented by the equation:

\begin{equation}\label{eq:cp04_stixel_movement_cost}
  c_m(u_i\{t\}, u_j\{t - 1\}) = 
  \begin{align*}
    \begin{cases}
    f_{cost}(u_i\{t\}, u_j\{t - 1\}) & \textbf{if} \text{ matching is applicable} \\
    \infty & \textbf{otherwise}
    \end{cases}
  \end{align*}
\end{equation}

Here, a matching is applicable if and only if the following restrictions are satisfied:
\begin{itemize}
 \item $|X(u_i\{t\}) - X(u_j\{t - 1\})| < \tau_{max\_disp}$, where the free parameter $\tau_{max\_disp}$ indicates the maximal displacement of a certain stixel between frames; and $X(u)$ refers to its $X$ position in 3d coordinates.
 \item $u_i\{t\}$ is not a new stixel. That is, this is not the very first frame in which the stixel appears.
 \item Stixels $u_i\{t\}$ and $u_j\{t - 1\}$ are not occluded.
 \item $f_{cost}(u_i\{t\}, u_j\{t - 1\}) < \tau_{max\_cost}$.
\end{itemize}

Else, the cost is considered as infinity, so the link is not included in the graph. $f_{cost}(u_i\{t\}, u_j\{t - 1\})$ is a cost function which can be composed by different cost metrics, weighted by a set of parameters in the form:

\begin{equation}\label{eq:cp04_cost_function}
\small
  \begin{align*}
  f_{cost}(u_i\{t\}, u_j\{t - 1\}) =~  & & \alpha_{SAD} & ~\cdot~ & f_{SAD}(u_i\{t\}, u_j\{t - 1\}) \\
      & + & \alpha_{hist} & ~\cdot~ & f_{hist}(u_i\{t\}, u_j\{t - 1\}) \\
      & + & \alpha_{height} & ~\cdot~ & f_{height}(u_i\{t\}, u_j\{t - 1\})
  \end{align*}
\end{equation}

Here, $\alpha_{SAD} + \alpha_{hist} + \alpha_{height} = 1$. $f_{SAD}(u_i\{t\}, u_j\{t - 1\})$, $f_{hist}(u_i\{t\}, u_j\{t - 1\})$ and $f_{height}(u_i\{t\}, u_j\{t - 1\})$ are the different cost metrics being tested, which are described next.

\subsubsection{\acf{SAD}}\label{ch:chapter04_01_03_01_01}

This cost is computed as the pixel-wise sum of the absolute differences over the RGB color scheme between $u_i\{t\}$ and $u_j\{t - 1\}$, following the expression

\begin{equation}\label{eq:cp04_stixel_movement_sad_cost}
\tiny
f_{SAD}(u_i\{t\}, u_j\{t - 1\}) = 
\overset{v_b(q_i\{t\})}{\underset{v_1=v_a(q_i\{t\})}{\sum}}
\overset{v_b(q_j\{t - 1\})}{\underset{v_2=v_a(q_i\{t - 1\})}{\sum}}
| I_L\{t\}(u_i\{t\}, v_1) - I_L\{t - 1\}(u_j\{t - 1\}, v_2) |
\end{equation}

As it is very unlikely for both stixels to have the exact same height, they are resized to a dimension of $30\,px$. This metric was originally used in \cite{gunyel2012stixels} for the computation of the movement of the stixels. It has been used in our tests to compare the our results with those obtained by them in their implementation.

\subsubsection{Histograms matching}\label{ch:chapter04_01_03_01_02}

As said, the same stixel has not always the same size in different frames. This can happen because the position of the object partially represented by the stixel can change its depth in the scene, or due to noise in the height detection of the stixel. In order to normalize this, we have computed the histogram of each of the stixels being compared. Then, the cost is computed from the Hellinger distance between both histograms:

\begin{equation}\label{eq:cp04_stixel_movement_histograms_cost}
f_{SAD}(u_i\{t\}, u_j\{t - 1\}) = d(P, Q) = 2 \cdot \sqrt { 1 - \underset{i=1}{\overset{d}{\sum}}\sqrt{P_i \cdot Q_i}}
\end{equation}

Here, $P$ is the histogram of the stixel $u_i\{t\}$, and $Q$ is the histogram of the stixel $u_j\{t - 1\}$. $d$ is the number of bins in the histogram. In our implementation, $d = 256$.

\subsubsection{Height difference}\label{ch:chapter04_01_03_01_05}

This metric is used to complement the other two metrics, as by its own is not enough to do a matching, but it is good enough to help in the decision, in case of very similar scores in two or more possible matches. $f_{height}$ is computed as follows:

\begin{equation}\label{eq:cp04_stixel_movement_height_cost}
f_{height}(u_i\{t\}, u_j\{t - 1\}) = 1 - |h(u_i\{t\} - h(u_j\{t - 1\})|
\end{equation}

, where $h(u)$ is the height of the stixel in column $u$. 

In our tests, we have tried different values for $\alpha_{SAD}$, $\alpha_{hist}$ and $\alpha_{height}$, obtaining the results shown in section \ref{ch:chapter04_02}, both in terms of performance and time. As said, the value obtained using the function $f_{cost}$ is used to weight the links between the nodes of a bipartite graph. This graph, whose nodes are the stixels at times $t$ and $t - 1$, is shown at figure \ref{fig:cp04_bipartite_graph}. In the top row, we can see the nodes (stixels) at current time, while in the lower row, previous stixels are represented. Costs of the match are represented by the edges, depicted in the image with the notation $\omega_{i,j}$.

\begin{figure}[h!]
\centering
\includegraphics{bipartite_graph}
\caption{Representation of the bipartite graph used for the matching process.}\label{fig:cp04_bipartite_graph}
\end{figure}

Over this graph, we solve the following maximization problem:

\comment{No estoy seguro de esta notación}
\begin{equation}\label{eq:cp04_match_minimization}
\mathcal{\hat{M}}=\underset{\mathcal{M}}{\arg\min} \underset{(i, j) \in \mathcal{M}}{\sum} \omega_{i,j}, 
~~~~\exists! (i, \cdot) \wedge \exists! (\cdot, j)
\end{equation}

This problem is solved using a $O(n \cdot m \cdot log(n))$ implementation of the Edmond's maximum weighted matching algorithm (\cite{edmonds1965paths}). The advantages of using this method instead the \ac{DP} implementation by \cite{gunyel2012stixels} is that we getter better times that the implementation available and, specially, we can ensure that each match is performed one-to-one. That is, in the \cite{gunyel2012stixels} implementation, the same stixel could be matched with more than one stixel from the other frame. This is not good, as we can have multiple paths from the same stixel, so it is difficult to know the trajectory it has followed in the past. In our implementation, we choose the matching set that maximizes the whole matching, ensuring that one stixel is matched with \emph{only} one stixel in the other frame.

\subsection{Obstacle-level tracking}\label{ch:chapter04_01_04}

In this stage, we perform the tracking at obstacle level, and not at stixel level, as described in the previous section. In the case of two-level tracking, we should have at this point the set of matchings $m(i,j) \in \mathcal{M}$, which relates each stixel at column $u_i$ in the frame $t$ with the stixel at column $u_j$ in the frame $t - 1$. Anyway, we do not really need to have these matches for this stage, and do the tracking directly at the obstacle level. This section comprises two steps: object detection (clustering) and object tracking.

\subsubsection{Clustering}\label{ch:chapter04_01_04_01}

From left to right, stixels are being evaluated using the algorithm shown at algorithm \ref{alg:cp04_clustering}.

\begin{algorithm}
\caption{Clustering algorithm}
\label{alg:cp04_clustering}
\begin{algorithmic}
\Function{Clustering}{$\mathcal{Q}\{t\}$}
  \State {$\mathcal{O} \gets \emptyset$}
  \State {$o \gets \emptyset$}
  \For {\textbf{each} stixel $q_i \in \mathcal{Q}$, from left to right}
    \If {$|depth(q_i) - depth(q_{i-1})| > \tau_{depth\_dist}$}
      \If {$|depth(q_i) - depth(q_{i-1})| > \tau_{depth\_dist}$}
	\State {$width(o) > \tau_{min\_width}$}
      \EndIf
      \State {$o \gets \emptyset$}
    \EndIf
    \State {$o \gets o \cup q_i$}
  \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

There, $\mathcal{Q}\{t\}$ is the set of stixels for the current frame $t$. From left to right, we start accumulating stixels, until the difference in depth between a certain pair of stixels is above a threshold $\tau_{depth\_dist}$, which is a user-defined free parameter. When this difference appears, we consider that we reached the right border of the obstacle, so it is added to the set $\mathcal{O}$, and a new obstacle is started. In case an obstacle is not wide enough, this obstacle is rejected. The reason for that is there are a stixel for each column, so there is a lot of noise that must be removed. This effect is easy to notice if we look at figure \ref{fig:cp04_stixels}.

At the end of the process, we will have got the set of obstacles $o_i \in \mathcal{O}$. Each obstacle has associated several parameters. One of them is the depth, which is computed as the minimal depth of all the associated stixels. In the left image of figure \ref{fig:cp04_clustering_aggregation}, the output obtained once this process has finished is shown.

\paragraph{Obstacle aggregation}\label{ch:chapter04_01_04_01_01}

In certain cases, due to a bad stixel detection, some stixels are located wrongly at a different depth from their real position. This effect happens, for example in cases like that shown at the left image of figure \ref{fig:cp04_clustering_aggregation}, in which a person in a fist plane opens his legs wide enough to show a big portion of the ground which is after him. This confuses the stixels detection algorithm, which thinks that the base of the obstacle is the central part of this person, and not his feet.

To solve this problem, the process described in algorithm \ref{alg:cp04_aggregation} is performed.

\begin{algorithm}
\caption{Aggregation algorithm}
\label{alg:cp04_aggregation}
\begin{algorithmic}
\Function{Aggregation}{$\mathcal{O}$}
  \State {$\mathcal{O'} \gets \emptyset$}
  \State {$o' \gets \emptyset$}
  \For {\textbf{each} object $o_i \in \mathcal{O}$, from left to right}
    \If {$|X(o_i) - X(o_{i-1})| > \tau_{lateral\_aggregation\_dist}$ \textbf{or}\\ \indent\indent\indent
	 $|Z(o_i) - Z(o_{i-1})| > \tau_{depth\_dist}$ \indent\indent~}

      \State {$\mathcal{O} \gets \mathcal{O} \cup o'$}
      \State {$o' \gets \emptyset$}
    \EndIf
    \State {$o' \gets o' \cup o$}
  \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

There, we test all obstacles previously detected, again from left to right. If the lateral distance (in world coordinates) is below a threshold $\tau_{lateral\_aggregation\_dist}$, we check again if the depth difference is below $\tau_{depth\_dist}$. If this condition is satisfied, both obstacles are joined together. In figure \ref{fig:cp04_clustering_aggregation}, an example of the output obtained from this process is shown. In the left image, the person in the first plane is divided into two different obstacles. After the aggregation process, this same person is assigned to a single obstacle. Depth of the obstacle is computed as the minimal depth of both original obstacles.

\begin{figure}[h!]
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{obstaclesBeforeAggregation}\label{fig:cp04_before_aggregation} &
\includegraphics[width=0.49\textwidth]{obstaclesAggregated}\label{fig:cp04_after_aggregation}
\end{tabular}
\caption{Comparison of the obstacles detected before and after the aggregation process.}\label{fig:cp04_clustering_aggregation}
\end{figure}

\paragraph{Obstacle filtering}\label{ch:chapter04_01_04_01_02}

If we look again at figure \ref{fig:cp04_clustering_aggregation}, we will notice that we are still detecting some fake obstacles. For instance, between the man with the red jacket and that with the dark suit, there are two of them. Next to the man in the second plane, also with a red jacket, there is another fake obstacle, and there is a last one in the right side of the image. Signs and poles are not considered fake obstacles, as they are elements to avoid, and we do not have a classification schema able to distinguish the class of the obstacle being detected.

In order to distinguish real obstacles from fake obstacles, we register the images obtained between frames, so we can distinguish the movement between frames. This movement can be originated both by the movement of the obstacle itself (i.e. a person walking), or by the movement of the camera respect to the obstacles. This makes occluded areas to appear, which allows detecting the border of obstacles that are not moving, but which we want to avoid. This is the case, for example, of the pole in the right part of the image.

\begin{framed}

The best way to do such a registration is through a polar rectification process (\cite{pollefeys1999simple}). This non-linear polar rectification process is able to register the images along the time, making it easier to detect the motion. The basis of this method is to reparameterize the images by setting the coordinates of their pixels in terms of the epipoles of a stereo pair. In this process, a linear transformation has to be found for every wedge part of the image with vertex at the epipole. This allows rectifying the whole image for every possible epipolar configuration, while keeping the size of resulting image reasonably large. The algorithm is designed in such way that no pixel loss is guaranteed. Also the length of original epipolar lines is preserved. The resulting image is upper-bounded by a size of $(2 (W + H) \times \sqrt{W^2 + H^2})$, where $W$ is the image width and $H$ its height. 

In order to reduce matching ambiguity to a half of the epipolar line in a case of epipole inside the image, it uses the concept of oriented epipolar geometry. To do this, one point correspondence is needed in both views. With this information, just positive coordinates over the epipolar line will be taken into account. The transfer of corresponding epipolar lines is obtained from the expression:

\begin{equation}\label{eq:cp04_epipolar_lines}
l_{t - 1} \sim H^{-T}l_t\text{, or }l_t \sim H^T l_{t - 1}
\end{equation}

Here, $l'$ is the epipolar line at image at frame $t$, and $l$ the corresponding epipolar line at frame $t - 1$. $H$ is an homography for an arbitrary plane, which can be obtained from the fundamental matrix $F$ (\cite{luong1996fundamental}):

\begin{equation}\label{eq:cp04_homography}
H = [e_{t - 1}] \times F + e_{t - 1}^T a
\end{equation}

, where $a$ is a random vector for which $det(H) \neq 0$, so that $H$ is invertible. $e$ is the epipole.

The first step in the polar rectification process consists on determine the common region for both images. For that purpose, we first need to compute the epipoles for both images and the homography $H$. For that purpose, we need to know first the fundamental matrix $F$. The epipolar geometry is described by the following equation:

\begin{equation}\label{eq:cp04_fundamental_matrix}
m_{L,t - 1}^T \cdot F \cdot m_{L,t}= 0
\end{equation}

, where $m_{L,t - 1}$ and $m_{L,t}$ are homogenous representations of corresponding image points in the left image of the frames $t$ and $t - 1$, respectively. So we need to compute these matches. To do that, we compute the correspondences between image pairs in the following order: $I_{L, t} \rightarrow I_{R, t} \rightarrow I_{R, t - 1} \rightarrow I_{L, t - 1} \rightarrow I_{L, t}$, where $I_{\{L,R\},t}$ is the left ($L$) or right ($R$) image at frame $t$. From a initial set of features in $I_{L, t}$, we get the valid matches in $I_{R, t}$, and the cycle is completed until we reach $I_{L, t}$ again, keeping just the valid matches. A match is valid if satisfies the following rules:
\begin{itemize}
 \item At the end of the cycle, points obtained should be the same as those from which we started the process. If not, a wrong match was found in the way.
 \item Features in $I_{L, t}$ must be in the same row as $I_{R, t}$. As images are rectified, the vertical component should be the same. Same applies to $I_{L, t - 1}$ and $I_{R, t - 1}$.
 \item The 2D distance between features from frame $t$ and $t - 1$ should not be too big, as the time between frames is not too big.
\end{itemize}

The result of this matching process is represented at figure \ref{fig:cp04_polar_fund_matrix_computation}. There, each matching cycle is represented by the same random color.

\begin{center}
\includegraphics{fundamentalMatrixComputation}
\captionof{figure}{Looking for the common points in frames $t$ and $t - 1$.}\label{fig:cp04_polar_fund_matrix_computation}
\end{center}

With these matches, we solve the system described by equation \ref{eq:cp04_fundamental_matrix}, obtaining $F$. From $F$, it is possible to get also the homography $H$ and the epipoles $e_t$ and $e_{t-1}$. So we can start looking for the common region between the images. This common region will be determined by the extremal epipolar lines, which will be those that touch the outer image corners. We must deal with three possible cases depending on
whether the epipoles are located inside or outside the image. These three cases are shown at figure \ref{fig:cp04_polar_common_region}.

\begin{center}
\includegraphics{polar_common_region}
\captionof{figure}{The three possible cases depending on the position of the epipoles.}\label{fig:cp04_polar_common_region}
\end{center}

There, $I_i^j$ refer to the extremal lines for each image, where $i$ indicates if it is the current ($t$) or the previous ($t - 1$) image, and $j$ is the corner with which the line intersects (1 and 2 for the current image and 3 and 4 for the previous one), so $I_i^j = [e_i] \times c_j$. If the epipole is located inside the image we need to decide which half-epipolar lines, pointing in oposite direction, is the correct one. 

\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{epipolarExtremeLeft1}\label{fig:cp04_epipolarExtremeLeft1} &
\includegraphics[width=0.49\textwidth]{epipolarExtremeRight1}\label{fig:cp04_epipolarExtremeRight1} \\
\includegraphics[width=0.49\textwidth]{epipolarExtremeLeft2}\label{fig:cp04_epipolarExtremeLeft2} &
\includegraphics[width=0.49\textwidth]{epipolarExtremeRight2}\label{fig:cp04_epipolarExtremeRight2} \\
\end{tabular}
\captionof{figure}{Two examples of the external epipolar lines found for a case in which the epipole is inside the image for both images (top row); and out, also in both images (bottom row).}\label{fig:cp04_epipolarExtreme}
\end{center}


In image \ref{fig:cp04_epipolarExtreme}, two examples of the outer epipolar lines found are shown. From these, the common region is defined, which will be represented by the beginning and ending epipolar lines $I_i^B$ and $I_i^E$. If both epipoles are inside the image, an arbitrary epipolar line can be used. In that case, we can avoid boundary effects by adding a small overlap. That is, we use a little bit more from $360\textdegree$.

Then, starting from $I_i^B$, we start constructing the rectified image, line by line, until we reach the epipolar line $I_i^E$. This process is repeated for $i=t$ and $i=t-1$ so, at the end of it, we will have both images rectified. In these images, each of the rows will have a correspondence with a certain epipolar line. The distance between two consecutive epipolar lines is determined independently for each pair of epipolar lines so we avoid pixel compression. The benefits of doing such non-linear warping are that this allows getting the smallest image without information loss.

Using this method, we compute a transformation map that allows to know the correspondence of the pixel of each image in euclidean and in polar coordinates easily. By using such a map, we computed a pair of images as that shown in the left side of figure \ref{fig:cp04_polarRectification_example}. Red line demonstrates the proper alignment of them. 

\begin{center}
  \begin{tabular}{cc}
  \includegraphics[width=0.6\textwidth]{polarRectification}\label{fig:cp04_polarRectification} &
  \includegraphics[width=0.3\textwidth]{polarDiff}\label{fig:cp04_polarDiff}
  \end{tabular}
  \captionof{figure}{Example of a pair of frames rectified. Red line shows that the alignment is correct. On the right, the absolute difference of both rectified images is shown.}
  \label{fig:cp04_polarRectification_example}
\end{center}

\end{framed}

For the discrimination between real and fake obstacles, we align the current frame at time $t$ with another frame at time $t - k$. Using the aligned images, we get the pixel-wise absolute difference of both images, so we can get those pixels for which there is movement. This difference image is projected back to coordinates of the current image. Then, this difference is thresholded and binarized, rejecting the small differences produced by noise. Result of this process is shown at the top of figure \ref{fig:cp04_obstacle_filtering}. Then, for each obstacle, we get the associated \ac{ROI}. For each ROI, we reject the top half of it, so we just look for movement in the area of the obstacle that is touching the ground. There are two reasons for that: the first one is that obstacles that move over the ground tend to have a higher movement in the lower part (for instance, legs or wheel movements). In the case of first plane static obstacles, the movement due to the camera change is more or less similar in the upper and lower half, so we are not loosing information. The second reason is that the stixels reconstruction assumes a planar ground in front of the camera, it is a fast way to discriminate bad correspondences.

\begin{figure}[h!]
  \begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thresholdedPolar}
    \end{minipage}\hfill~
  \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c|}
      \hline
      0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
      \hline
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi0} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi1} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi2} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi3} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi4} & 
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi5} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi6} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi7} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi8} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi9} &
      \includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/roi10} \\
      \hline
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ0}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ1}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ2}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ3}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ4}} & 
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ5}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ6}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ7}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ8}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{green}{green}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ9}} &
      \setlength{\fboxsep}{1pt}\fcolorbox{red}{red}{\includegraphics[width=0.15\textwidth, height=0.15\textwidth]{obstacleFilter/occ10}} \\
      \hline
    \end{tabular}
  \end{minipage}\hfill
  \caption{Object filtering process. In the top, the binarized motion image is shown. In the bottom the occupancy maps generated for each candidate obstacle.}\label{fig:cp04_obstacle_filtering}
\end{figure}

% TODO
% Decir que cogemos cada ROI y dividimos el w y h correspondientes a coordenadas del mundo real y las dividimos en celdas de 10x10, obteniendo el grid que se muestra en la figura XXX
% Si el porcentaje de celdas marcadas es > umbral, el obstáculo es real
% Si no, se elimina
% También filtramos por tamaño del obstáculo en coordenadas reales

In our tests, we decided to use $k = 0.2\,s$, in order to be sure that differences are appreciable, but being conservative at the same time.


% Imagen con las diferencias reproyectadas en la imagen I_L,t

\subsubsection{Tracking}\label{ch:chapter04_01_04_02}
\paragraph{Two-level tracking case}\label{ch:chapter04_01_04_02_01}
\paragraph{Object tracking case}\label{ch:chapter04_01_04_02_02}

\subsection{Kalman filtering}\label{ch:chapter04_01_05}

\section{Results}\label{ch:chapter04_02}

\subsection{Clustering}\label{ch:chapter04_02_01}

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{detectionRate}
\caption{Obstacles detection rate achieved using our clustering method.}\label{fig:cp04_detection_rate}
\end{figure}

\begin{figure}[h!]
\begin{tabular}{cc}
\includegraphics[width=0.49\textwidth]{stixelsDetection}\label{fig:cp04_stixels_detection} &
\includegraphics[width=0.49\textwidth]{obstacleDetection}\label{fig:cp04_obstacle_detection}
\end{tabular}
\caption{Comparison of the stixels obtained trough the method of \cite{benenson2012pedestrian} and the clustering performed in our method.}\label{fig:cp04_clustering_comparison}
\end{figure}

\subsection{Stixel accuracy}\label{ch:chapter04_02_02}

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{disparity}
\caption{Difference in disparity achieved for both our clustering method and the initial stixel reconstruction.}\label{fig:cp04_disparity_comparison}
\end{figure}

\begin{figure}[h!]
\begin{tabular}{cccc}
% \centering
\includegraphics[width=0.25\textwidth]{elas}\label{fig:cp04_reconstruction_elas} & 
\includegraphics[width=0.25\textwidth]{stixels}\label{fig:cp04_reconstruction_stixels} & 
\includegraphics[width=0.25\textwidth]{objects}\label{fig:cp04_reconstruction_objects} &
\includegraphics[height=0.1875\textwidth]{colorscale_jet}\label{fig:cp04_reconstruction_colorscale} % 0.25*48/64 = 0.1875
\end{tabular}
\caption{Comparison of the disparities obtained for \ac{ELAS}, stixels and the reconstructed objects.}\label{fig:cp04_reconstruction}
\end{figure}

\subsection{Tracking}\label{ch:chapter04_02_03}

\subsubsection{Performance along the sequence}\label{ch:chapter04_02_03_01}

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{recall_vs_delta_frames}
\caption{Comparison of the recall obtained for the different configurations.}\label{fig:cp04_recall_vs_delta_frames}
\end{figure}

\subsubsection{Performance at different frame increments}\label{ch:chapter04_02_03_02}

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{increments}
\caption{Comparison of the recall obtained for the different frame increments.}\label{fig:cp04_increments}
\end{figure}

\begin{figure}[h!]
\begin{tabular}{cc}
% \centering
\includegraphics[width=0.45\textwidth,trim=50 40 80 60,clip]{recall_vs_delta_frames_vs_step_test28b}\label{fig:cp04_recall_vs_delta_frames_vs_step_test28b} &
\includegraphics[width=0.45\textwidth,trim=50 40 80 60,clip]{recall_vs_delta_frames_vs_step_test16b}\label{fig:cp04_recall_vs_delta_frames_vs_step_test16b}
\end{tabular}
\caption{Comparison of the tracking capabilities at different frame increments for configurations \todo{XXX and XXX}.}\label{fig:cp04_recall_vs_delta_frames_vs_step}
\end{figure}

\subsection{Computation time}\label{ch:chapter04_02_04}

\begin{figure}[h!]
\centering
\includegraphics[trim=50 40 80 60,clip]{times_average}
\caption{Times obtained for each configuration.}\label{fig:cp04_times_average}
\end{figure}

% No hay enlaces para stixels nuevos
% No hay enlaces con un coste > tanto


\section{Summary}\label{ch:chapter04_07}

% Trabajo futuro: Utilizar el polar calibration para la generación de los propios stixels?
