%%
%%  appendixSVM.tex - Obstacle Detection and Planning for Autonomous Vehicles based on Computer Vision Techniques
%%
%%  Copyright 2014 NÃ©stor Morales <nestor@isaatc.ull.es>
%%
%%  This work is licensed under a Creative Commons Attribution 4.0 International License.
%%

\graphicspath{{./images/chapter06/bmps/}{./images/chapter06/vects/}{./images/chapter06/}}

\chapter{Support Vector Machines}\label{ch:appendixSVM}

\acfp{SVM} are a set of linear classifiers usually employed both for classification and regression, which are based on methods of supervised learning. This technique is based on the Vapnik Chervonenkis (VC) dimension from the statistical learning theory and Structural Risk Minimization.
The original algorithm was developed by Vladimir Vapnik in 1963 as a linear classifier, but the most currently used version (soft margin) was proposed together by Vapnik and Corinna Cortes in 1995 \citep{cortes1995support}.

\section{Margin Maximization}\label{ch:appendixSVM_01}

Let $(x_1,y_1),\dots,(x_N,y_N)$, with $\begin{array}{l c r}  
  x_k \in \mathbb{R}^m & y_k \in {-1,1} & k=1,\dots,N 
\end{array}$ be the set of samples separated by the hyperplane $w^T \cdot x + b = 0$, where $x$ is a $N$-dimensional point and $w$ is a vector of weights. This hyperplane divides data, in the way that $w^T \cdot x_k + b > 0$ for the $x_k$ in a class, and $w^T \cdot x_k + b < 0$ for the $x_k$ in the other class. If data is separable, it is quite likely to have more than one way to do that. From all possible solutions, the \ac{SVM} selects that with the biggest distance from the hyperplane to the nearest data points. The described method takes advantage of this fact.
If training points are labeled in the way that $y_k \in \{ -1, +1\}$, where $+1$ is a positive sample, and $-1$ a negative sample, the following expression is obtained:

\begin{equation}\label{eq:appendixSVM_hyperplane_scaled}
  \begin{array}{l r}
    y_k (w^T \cdot x_k + b) \geq 1 & \forall k \in \{1, \dots, N \}
  \end{array}
\end{equation}

Based on this, both classes can be separated by two hyperplanes, $H_1: w^T \cdot x_k + b = 1$ and $H_2: w^T \cdot x_k + b = -1$, in the way that there are not data points inside the gap existing between $H_1$ and $H_2$. The distance between them is $2 / \|w\|$, where $\|w\|$ is the Euclidean norm of $w$. Therefore, the way in which the parameters that maximize this distance are obtained is by minimizing the following objective function:

\begin{equation}\label{eq:appendixSVM_objective_function}
 L(w) = {{\|w\|^2} \over 2}
\end{equation}

, such that $\begin{array}{l r} 
y_k (w^T \cdot x_k + b) \geq 1 & \forall k \in \{1, \dots, N \}
\end{array} $.

To solve this, the dual problem must be obtained. After a few calculations, we obtain the following formulation:

\begin{equation}\label{eq:appendixSVM_dual_formulation}
 L_D (\alpha) = \sum_{k=1}^N \alpha_k - {1 \over 2} \sum_{k = 1, l=1}^N \alpha_k \alpha_l y_k y_l x_k^T x_l
\end{equation}

$L_D$ must be maximized. The training vectors $x_k$ with positive Lagrange multipliers $\alpha_k$ in any of the two hyperplanes are called Support Vectors (SV). SVs define the hyperplane parameters. Therefore, using the SVs it is possible to define a discriminative function like the next one: 

\begin{equation}\label{eq:appendixSVM_discriminative_function}
 y = sign(w^T \cdot x + b) = sign \left(\sum_{k \in S} \alpha_k y_k x_k^T x + b \right)
\end{equation}

, where $S$ is the set containing all SVs.

\section{Soft Margin}\label{ch:appendixSVM_02}

In a real problem, it is very unlikely that a line divides data in an exact way. In fact, most of the times it is not desired to divide them accurately, even having the possibility of defining a non-linear decision boundary. 
In these cases, the objective function \ref{eq:appendixSVM_objective_function} can be substituted by:

\begin{equation}\label{eq:appendixSVM_soft_objective_function}
 L(w) = {{\|w\|^2} \over 2} + C \sum_{k=1}^N \xi_k
\end{equation}

, where $\xi_k$ is the error allowed in the classification.

The solution of this problem can also be obtained through the dual formulation, as we did for the linear case. The non-zero $\alpha_k$ corresponds to the next two cases:
\begin{itemize}
 \item For SVs on either of two hyperplanes, $0 < \alpha_k < C$.
 \item For SVs inside the hyperplanes, $\alpha_k = C $.
\end{itemize}

\section{Non-linear \ac{SVM}}\label{ch:appendixSVM_03}

\acp{SVM} can also be used to determine non-linear separable surfaces. To do that, it is possible to use kernel based techniques. Using the appropriate kernel function we can find the separating hyperplane by using higher dimensions. This kernel function allows changing the problem of finding a non-linear separating surface in the original space by computing a separating hyperplane inside the high-dimensional space, allowing reducing the calculation costs and, at the same time, obtaining an optimal non-linear separating surface inside the original space, as described at \cite{burges1998tutorial}.

Let $\pi$ be the mapping of the data from the original space to higher dimensions. This mapping is given by several functions $K$:

\begin{equation}\label{eq:appendixSVM_kernel_func}
 K(x_1, x_2) = \pi(x_1)^T \cdot \pi(x_2)
\end{equation}

Using this kernel function, the objective function in \ref{eq:appendixSVM_dual_formulation} becomes:

\begin{equation}\label{eq:appendixSVM_soft_formulation}
 L_d(\alpha) = \sum_{k=1}^N  \alpha_k - {1 \over 2} \sum_{k = 1, l=1}^N \alpha_k \alpha_l y_k y_l K(x_k, x_l)
\end{equation}

And the discriminative function:

\begin{equation}\label{eq:appendixSVM_soft_discriminative_function}
 y = sign \left( \sum_{k \in S} \alpha_k y_k K(x_k, x) + b \right)
\end{equation}

Function $K$ can take several forms, being the most common those listed next:
\begin{itemize}
 \item Linear: $K(x_k, x_l) = x_k^T \cdot x_l$.
 \item Polynomial: $K(x_k, x_l) = (\gamma \cdot x_k^T \cdot x_l + r)^d$.
 \item \acf{RBF}: $K(x_k, x_l) = \exp(-\gamma \cdot \| x_k - x_l\|)$.
 \item Sigmoid: $K(x_k, x_l) = \tanh(\gamma \cdot x_k^T \cdot x_l + r)$
\end{itemize}

In the method described in chapter \ref{ch:chapter06}, we use the Gaussian \acf{RBF}, as it is able to produce continuous soft decision boundaries, making it the most suitable for application.